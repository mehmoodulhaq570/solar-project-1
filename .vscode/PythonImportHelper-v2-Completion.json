[
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "streamlit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "streamlit",
        "description": "streamlit",
        "detail": "streamlit",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "AVAILABLE_MODELS",
        "importPath": "frontend.config",
        "description": "frontend.config",
        "isExtraImport": true,
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODELS",
        "importPath": "frontend.config",
        "description": "frontend.config",
        "isExtraImport": true,
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "MODEL_COLORS",
        "importPath": "frontend.config",
        "description": "frontend.config",
        "isExtraImport": true,
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "LOCATION_NAME",
        "importPath": "frontend.config",
        "description": "frontend.config",
        "isExtraImport": true,
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "importPath": "frontend.config",
        "description": "frontend.config",
        "isExtraImport": true,
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "fetch_api_forecast",
        "importPath": "frontend.api",
        "description": "frontend.api",
        "isExtraImport": true,
        "detail": "frontend.api",
        "documentation": {}
    },
    {
        "label": "load_traditional_models",
        "importPath": "frontend.models",
        "description": "frontend.models",
        "isExtraImport": true,
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "load_keras_models",
        "importPath": "frontend.models",
        "description": "frontend.models",
        "isExtraImport": true,
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "load_pytorch_models",
        "importPath": "frontend.models",
        "description": "frontend.models",
        "isExtraImport": true,
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "load_scalers",
        "importPath": "frontend.models",
        "description": "frontend.models",
        "isExtraImport": true,
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "TENSORFLOW_AVAILABLE",
        "importPath": "frontend.models",
        "description": "frontend.models",
        "isExtraImport": true,
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "PYTORCH_AVAILABLE",
        "importPath": "frontend.models",
        "description": "frontend.models",
        "isExtraImport": true,
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "XGBOOST_AVAILABLE",
        "importPath": "frontend.models",
        "description": "frontend.models",
        "isExtraImport": true,
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "load_historical_data",
        "importPath": "frontend.data",
        "description": "frontend.data",
        "isExtraImport": true,
        "detail": "frontend.data",
        "documentation": {}
    },
    {
        "label": "get_data_summary",
        "importPath": "frontend.data",
        "description": "frontend.data",
        "isExtraImport": true,
        "detail": "frontend.data",
        "documentation": {}
    },
    {
        "label": "PredictionEngine",
        "importPath": "frontend.prediction",
        "description": "frontend.prediction",
        "isExtraImport": true,
        "detail": "frontend.prediction",
        "documentation": {}
    },
    {
        "label": "create_forecast_plots",
        "importPath": "frontend.visualization",
        "description": "frontend.visualization",
        "isExtraImport": true,
        "detail": "frontend.visualization",
        "documentation": {}
    },
    {
        "label": "create_deep_learning_comparison_plot",
        "importPath": "frontend.visualization",
        "description": "frontend.visualization",
        "isExtraImport": true,
        "detail": "frontend.visualization",
        "documentation": {}
    },
    {
        "label": "calculate_metrics",
        "importPath": "frontend.visualization",
        "description": "frontend.visualization",
        "isExtraImport": true,
        "detail": "frontend.visualization",
        "documentation": {}
    },
    {
        "label": "create_model_architecture_info",
        "importPath": "frontend.visualization",
        "description": "frontend.visualization",
        "isExtraImport": true,
        "detail": "frontend.visualization",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "joblib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "joblib",
        "description": "joblib",
        "detail": "joblib",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "mean_absolute_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "r2_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_absolute_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "r2_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_absolute_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "r2_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_absolute_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "r2_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_absolute_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "r2_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_absolute_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "r2_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "openmeteo_requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openmeteo_requests",
        "description": "openmeteo_requests",
        "detail": "openmeteo_requests",
        "documentation": {}
    },
    {
        "label": "requests_cache",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests_cache",
        "description": "requests_cache",
        "detail": "requests_cache",
        "documentation": {}
    },
    {
        "label": "retry",
        "importPath": "retry_requests",
        "description": "retry_requests",
        "isExtraImport": true,
        "detail": "retry_requests",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "lightning",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "lightning",
        "description": "lightning",
        "detail": "lightning",
        "documentation": {}
    },
    {
        "label": "EarlyStopping",
        "importPath": "lightning.pytorch.callbacks",
        "description": "lightning.pytorch.callbacks",
        "isExtraImport": true,
        "detail": "lightning.pytorch.callbacks",
        "documentation": {}
    },
    {
        "label": "ModelCheckpoint",
        "importPath": "lightning.pytorch.callbacks",
        "description": "lightning.pytorch.callbacks",
        "isExtraImport": true,
        "detail": "lightning.pytorch.callbacks",
        "documentation": {}
    },
    {
        "label": "LearningRateMonitor",
        "importPath": "lightning.pytorch.callbacks",
        "description": "lightning.pytorch.callbacks",
        "isExtraImport": true,
        "detail": "lightning.pytorch.callbacks",
        "documentation": {}
    },
    {
        "label": "EarlyStopping",
        "importPath": "lightning.pytorch.callbacks",
        "description": "lightning.pytorch.callbacks",
        "isExtraImport": true,
        "detail": "lightning.pytorch.callbacks",
        "documentation": {}
    },
    {
        "label": "ModelCheckpoint",
        "importPath": "lightning.pytorch.callbacks",
        "description": "lightning.pytorch.callbacks",
        "isExtraImport": true,
        "detail": "lightning.pytorch.callbacks",
        "documentation": {}
    },
    {
        "label": "LearningRateMonitor",
        "importPath": "lightning.pytorch.callbacks",
        "description": "lightning.pytorch.callbacks",
        "isExtraImport": true,
        "detail": "lightning.pytorch.callbacks",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "LitTFT",
        "importPath": "tft",
        "description": "tft",
        "isExtraImport": true,
        "detail": "tft",
        "documentation": {}
    },
    {
        "label": "LitTFT",
        "importPath": "tft",
        "description": "tft",
        "isExtraImport": true,
        "detail": "tft",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "LitTCN",
        "importPath": "tcn",
        "description": "tcn",
        "isExtraImport": true,
        "detail": "tcn",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "layers",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "layers",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "layers",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "callbacks",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "backend",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "xgboost",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xgboost",
        "description": "xgboost",
        "detail": "xgboost",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "tensorflow.keras.models",
        "description": "tensorflow.keras.models",
        "isExtraImport": true,
        "detail": "tensorflow.keras.models",
        "documentation": {}
    },
    {
        "label": "fetch_openmeteo_solar_forecast",
        "kind": 2,
        "importPath": "frontend.api",
        "description": "frontend.api",
        "peekOfCode": "def fetch_openmeteo_solar_forecast(\n    year: int, month: int, day: int\n) -> Optional[List[float]]:\n    \"\"\"\n    Fetch solar radiation forecast from Open-Meteo API.\n    Args:\n        year: Forecast year\n        month: Forecast month\n        day: Forecast day\n    Returns:",
        "detail": "frontend.api",
        "documentation": {}
    },
    {
        "label": "fetch_nasa_power_solar",
        "kind": 2,
        "importPath": "frontend.api",
        "description": "frontend.api",
        "peekOfCode": "def fetch_nasa_power_solar(year: int, month: int, day: int) -> Optional[List[float]]:\n    \"\"\"\n    Fetch solar radiation data from NASA POWER API.\n    Returns hourly ALLSKY_SFC_SW_DWN (Surface Shortwave Downward Irradiance) in W/mÂ².\n    Note: Historical data only (~1 week delay)\n    Args:\n        year: Data year\n        month: Data month\n        day: Data day\n    Returns:",
        "detail": "frontend.api",
        "documentation": {}
    },
    {
        "label": "fetch_api_forecast",
        "kind": 2,
        "importPath": "frontend.api",
        "description": "frontend.api",
        "peekOfCode": "def fetch_api_forecast(api_name: str, year: int, month: int, day: int) -> tuple:\n    \"\"\"\n    Fetch forecast from selected API with fallback.\n    Args:\n        api_name: \"NASA POWER\" or \"Open-Meteo\"\n        year, month, day: Forecast date\n    Returns:\n        Tuple of (hourly_data, actual_api_name, use_openmeteo_weights)\n    \"\"\"\n    use_openmeteo_weights = False",
        "detail": "frontend.api",
        "documentation": {}
    },
    {
        "label": "selected_date",
        "kind": 5,
        "importPath": "frontend.app",
        "description": "frontend.app",
        "peekOfCode": "selected_date = st.sidebar.date_input(\n    \"ðŸ“… Forecast Date\",\n    datetime.today(),\n    help=\"Select the date for solar radiation forecast\",\n)\n# Model folder paths\ntraditional_folder = os.path.join(BASE_DIR, \"saved_models\")\nlstm_folder = os.path.join(BASE_DIR, \"saved_models\")  # LSTM models are in saved_models\ntft_folder = os.path.join(BASE_DIR, \"saved_models_tft\")\n# Model selection with availability indicators",
        "detail": "frontend.app",
        "documentation": {}
    },
    {
        "label": "traditional_folder",
        "kind": 5,
        "importPath": "frontend.app",
        "description": "frontend.app",
        "peekOfCode": "traditional_folder = os.path.join(BASE_DIR, \"saved_models\")\nlstm_folder = os.path.join(BASE_DIR, \"saved_models\")  # LSTM models are in saved_models\ntft_folder = os.path.join(BASE_DIR, \"saved_models_tft\")\n# Model selection with availability indicators\nst.sidebar.markdown(\"### ðŸ¤– Model Selection\")\n# Check model availability - check both saved_models and saved_models_lstm\navailable_model_status = {\n    \"XGBoost\": XGBOOST_AVAILABLE\n    and os.path.exists(os.path.join(traditional_folder, \"xgboost_model.pkl\")),\n    \"Random Forest\": os.path.exists(",
        "detail": "frontend.app",
        "documentation": {}
    },
    {
        "label": "lstm_folder",
        "kind": 5,
        "importPath": "frontend.app",
        "description": "frontend.app",
        "peekOfCode": "lstm_folder = os.path.join(BASE_DIR, \"saved_models\")  # LSTM models are in saved_models\ntft_folder = os.path.join(BASE_DIR, \"saved_models_tft\")\n# Model selection with availability indicators\nst.sidebar.markdown(\"### ðŸ¤– Model Selection\")\n# Check model availability - check both saved_models and saved_models_lstm\navailable_model_status = {\n    \"XGBoost\": XGBOOST_AVAILABLE\n    and os.path.exists(os.path.join(traditional_folder, \"xgboost_model.pkl\")),\n    \"Random Forest\": os.path.exists(\n        os.path.join(traditional_folder, \"random_forest_model.pkl\")",
        "detail": "frontend.app",
        "documentation": {}
    },
    {
        "label": "tft_folder",
        "kind": 5,
        "importPath": "frontend.app",
        "description": "frontend.app",
        "peekOfCode": "tft_folder = os.path.join(BASE_DIR, \"saved_models_tft\")\n# Model selection with availability indicators\nst.sidebar.markdown(\"### ðŸ¤– Model Selection\")\n# Check model availability - check both saved_models and saved_models_lstm\navailable_model_status = {\n    \"XGBoost\": XGBOOST_AVAILABLE\n    and os.path.exists(os.path.join(traditional_folder, \"xgboost_model.pkl\")),\n    \"Random Forest\": os.path.exists(\n        os.path.join(traditional_folder, \"random_forest_model.pkl\")\n    ),",
        "detail": "frontend.app",
        "documentation": {}
    },
    {
        "label": "available_model_status",
        "kind": 5,
        "importPath": "frontend.app",
        "description": "frontend.app",
        "peekOfCode": "available_model_status = {\n    \"XGBoost\": XGBOOST_AVAILABLE\n    and os.path.exists(os.path.join(traditional_folder, \"xgboost_model.pkl\")),\n    \"Random Forest\": os.path.exists(\n        os.path.join(traditional_folder, \"random_forest_model.pkl\")\n    ),\n    \"LSTM\": TENSORFLOW_AVAILABLE\n    and (\n        os.path.exists(os.path.join(traditional_folder, \"lstm_model.h5\"))\n        or os.path.exists(os.path.join(BASE_DIR, \"saved_models_lstm\", \"lstm_model.h5\"))",
        "detail": "frontend.app",
        "documentation": {}
    },
    {
        "label": "selectable_models",
        "kind": 5,
        "importPath": "frontend.app",
        "description": "frontend.app",
        "peekOfCode": "selectable_models = [\n    m for m in AVAILABLE_MODELS if available_model_status.get(m, False)\n]\ndefault_selection = [m for m in DEFAULT_MODELS if m in selectable_models]\nselected_models = st.sidebar.multiselect(\n    \"Select Models\",\n    selectable_models,\n    default=default_selection,\n    help=\"Choose which models to use for prediction\",\n)",
        "detail": "frontend.app",
        "documentation": {}
    },
    {
        "label": "default_selection",
        "kind": 5,
        "importPath": "frontend.app",
        "description": "frontend.app",
        "peekOfCode": "default_selection = [m for m in DEFAULT_MODELS if m in selectable_models]\nselected_models = st.sidebar.multiselect(\n    \"Select Models\",\n    selectable_models,\n    default=default_selection,\n    help=\"Choose which models to use for prediction\",\n)\nst.sidebar.markdown(\"---\")\n# API Comparison toggle\nenable_api = st.sidebar.checkbox(",
        "detail": "frontend.app",
        "documentation": {}
    },
    {
        "label": "selected_models",
        "kind": 5,
        "importPath": "frontend.app",
        "description": "frontend.app",
        "peekOfCode": "selected_models = st.sidebar.multiselect(\n    \"Select Models\",\n    selectable_models,\n    default=default_selection,\n    help=\"Choose which models to use for prediction\",\n)\nst.sidebar.markdown(\"---\")\n# API Comparison toggle\nenable_api = st.sidebar.checkbox(\n    \"ðŸŒ Compare with API\",",
        "detail": "frontend.app",
        "documentation": {}
    },
    {
        "label": "enable_api",
        "kind": 5,
        "importPath": "frontend.app",
        "description": "frontend.app",
        "peekOfCode": "enable_api = st.sidebar.checkbox(\n    \"ðŸŒ Compare with API\",\n    value=True,\n    help=\"Fetch API forecast for comparison\",\n)\n# API Selection\nselected_api = st.sidebar.selectbox(\n    \"ðŸ“¡ Select API\",\n    [\"NASA POWER\", \"Open-Meteo\"],\n    index=0,",
        "detail": "frontend.app",
        "documentation": {}
    },
    {
        "label": "selected_api",
        "kind": 5,
        "importPath": "frontend.app",
        "description": "frontend.app",
        "peekOfCode": "selected_api = st.sidebar.selectbox(\n    \"ðŸ“¡ Select API\",\n    [\"NASA POWER\", \"Open-Meteo\"],\n    index=0,\n    help=\"NASA POWER: Historical data (same source as training, ~1 week delay)\\nOpen-Meteo: Real-time forecast (today + 16 days)\",\n    disabled=not enable_api,\n)\n# Show API info\nif enable_api:\n    if selected_api == \"NASA POWER\":",
        "detail": "frontend.app",
        "documentation": {}
    },
    {
        "label": "LATITUDE",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "LATITUDE = 31.56\nLONGITUDE = 74.35\nTIMEZONE = \"Asia/Karachi\"\nLOCATION_NAME = \"Lahore, Pakistan\"\n# ============== Model Configuration ==============\nSEQUENCE_LENGTH = 24\nMAX_LAG = 24\nHIDDEN_SIZE = 64\nNUM_HEADS = 4\nNUM_LAYERS_TFT = 2",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "LONGITUDE",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "LONGITUDE = 74.35\nTIMEZONE = \"Asia/Karachi\"\nLOCATION_NAME = \"Lahore, Pakistan\"\n# ============== Model Configuration ==============\nSEQUENCE_LENGTH = 24\nMAX_LAG = 24\nHIDDEN_SIZE = 64\nNUM_HEADS = 4\nNUM_LAYERS_TFT = 2\nNUM_LAYERS_TCN = 4",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "TIMEZONE",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "TIMEZONE = \"Asia/Karachi\"\nLOCATION_NAME = \"Lahore, Pakistan\"\n# ============== Model Configuration ==============\nSEQUENCE_LENGTH = 24\nMAX_LAG = 24\nHIDDEN_SIZE = 64\nNUM_HEADS = 4\nNUM_LAYERS_TFT = 2\nNUM_LAYERS_TCN = 4\nKERNEL_SIZE_TCN = 3",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "LOCATION_NAME",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "LOCATION_NAME = \"Lahore, Pakistan\"\n# ============== Model Configuration ==============\nSEQUENCE_LENGTH = 24\nMAX_LAG = 24\nHIDDEN_SIZE = 64\nNUM_HEADS = 4\nNUM_LAYERS_TFT = 2\nNUM_LAYERS_TCN = 4\nKERNEL_SIZE_TCN = 3\n# ============== Model Folders ==============",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "SEQUENCE_LENGTH",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "SEQUENCE_LENGTH = 24\nMAX_LAG = 24\nHIDDEN_SIZE = 64\nNUM_HEADS = 4\nNUM_LAYERS_TFT = 2\nNUM_LAYERS_TCN = 4\nKERNEL_SIZE_TCN = 3\n# ============== Model Folders ==============\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nDATA_PATH = os.path.join(",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "MAX_LAG",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "MAX_LAG = 24\nHIDDEN_SIZE = 64\nNUM_HEADS = 4\nNUM_LAYERS_TFT = 2\nNUM_LAYERS_TCN = 4\nKERNEL_SIZE_TCN = 3\n# ============== Model Folders ==============\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nDATA_PATH = os.path.join(\n    BASE_DIR, \"NASA meteriological and solar radiaton data\", \"lahore_hourly_filled.csv\"",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "HIDDEN_SIZE",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "HIDDEN_SIZE = 64\nNUM_HEADS = 4\nNUM_LAYERS_TFT = 2\nNUM_LAYERS_TCN = 4\nKERNEL_SIZE_TCN = 3\n# ============== Model Folders ==============\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nDATA_PATH = os.path.join(\n    BASE_DIR, \"NASA meteriological and solar radiaton data\", \"lahore_hourly_filled.csv\"\n)",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "NUM_HEADS",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "NUM_HEADS = 4\nNUM_LAYERS_TFT = 2\nNUM_LAYERS_TCN = 4\nKERNEL_SIZE_TCN = 3\n# ============== Model Folders ==============\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nDATA_PATH = os.path.join(\n    BASE_DIR, \"NASA meteriological and solar radiaton data\", \"lahore_hourly_filled.csv\"\n)\nMODEL_FOLDERS = {",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "NUM_LAYERS_TFT",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "NUM_LAYERS_TFT = 2\nNUM_LAYERS_TCN = 4\nKERNEL_SIZE_TCN = 3\n# ============== Model Folders ==============\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nDATA_PATH = os.path.join(\n    BASE_DIR, \"NASA meteriological and solar radiaton data\", \"lahore_hourly_filled.csv\"\n)\nMODEL_FOLDERS = {\n    \"Traditional Models\": os.path.join(BASE_DIR, \"saved_models\"),",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "NUM_LAYERS_TCN",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "NUM_LAYERS_TCN = 4\nKERNEL_SIZE_TCN = 3\n# ============== Model Folders ==============\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nDATA_PATH = os.path.join(\n    BASE_DIR, \"NASA meteriological and solar radiaton data\", \"lahore_hourly_filled.csv\"\n)\nMODEL_FOLDERS = {\n    \"Traditional Models\": os.path.join(BASE_DIR, \"saved_models\"),\n    \"LSTM Models\": os.path.join(BASE_DIR, \"saved_models_lstm\"),",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "KERNEL_SIZE_TCN",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "KERNEL_SIZE_TCN = 3\n# ============== Model Folders ==============\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nDATA_PATH = os.path.join(\n    BASE_DIR, \"NASA meteriological and solar radiaton data\", \"lahore_hourly_filled.csv\"\n)\nMODEL_FOLDERS = {\n    \"Traditional Models\": os.path.join(BASE_DIR, \"saved_models\"),\n    \"LSTM Models\": os.path.join(BASE_DIR, \"saved_models_lstm\"),\n    \"TFT/TCN Models\": os.path.join(BASE_DIR, \"saved_models_tft\"),",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nDATA_PATH = os.path.join(\n    BASE_DIR, \"NASA meteriological and solar radiaton data\", \"lahore_hourly_filled.csv\"\n)\nMODEL_FOLDERS = {\n    \"Traditional Models\": os.path.join(BASE_DIR, \"saved_models\"),\n    \"LSTM Models\": os.path.join(BASE_DIR, \"saved_models_lstm\"),\n    \"TFT/TCN Models\": os.path.join(BASE_DIR, \"saved_models_tft\"),\n}\n# Default model folder",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "DATA_PATH",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "DATA_PATH = os.path.join(\n    BASE_DIR, \"NASA meteriological and solar radiaton data\", \"lahore_hourly_filled.csv\"\n)\nMODEL_FOLDERS = {\n    \"Traditional Models\": os.path.join(BASE_DIR, \"saved_models\"),\n    \"LSTM Models\": os.path.join(BASE_DIR, \"saved_models_lstm\"),\n    \"TFT/TCN Models\": os.path.join(BASE_DIR, \"saved_models_tft\"),\n}\n# Default model folder\nDEFAULT_MODEL_FOLDER = \"saved_models\"",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "MODEL_FOLDERS",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "MODEL_FOLDERS = {\n    \"Traditional Models\": os.path.join(BASE_DIR, \"saved_models\"),\n    \"LSTM Models\": os.path.join(BASE_DIR, \"saved_models_lstm\"),\n    \"TFT/TCN Models\": os.path.join(BASE_DIR, \"saved_models_tft\"),\n}\n# Default model folder\nDEFAULT_MODEL_FOLDER = \"saved_models\"\n# ============== Available Models ==============\nAVAILABLE_MODELS = [\n    \"XGBoost\",",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOLDER",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "DEFAULT_MODEL_FOLDER = \"saved_models\"\n# ============== Available Models ==============\nAVAILABLE_MODELS = [\n    \"XGBoost\",\n    \"Random Forest\",\n    \"LSTM\",\n    \"CNN-LSTM\",\n    \"TFT\",\n    \"TCN\",\n    \"Ensemble\",",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "AVAILABLE_MODELS",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "AVAILABLE_MODELS = [\n    \"XGBoost\",\n    \"Random Forest\",\n    \"LSTM\",\n    \"CNN-LSTM\",\n    \"TFT\",\n    \"TCN\",\n    \"Ensemble\",\n]\n# Default selected models",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODELS",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "DEFAULT_MODELS = [\"XGBoost\", \"Random Forest\", \"TFT\", \"Ensemble\"]\n# ============== Ensemble Configuration ==============\n# NASA POWER API weights (optimized for NASA satellite data)\nNASA_ENSEMBLE_WEIGHTS = {\n    \"XGBoost\": 0.30,\n    \"RandomForest\": 0.25,\n    \"LSTM\": 0.08,\n    \"CNN-LSTM\": 0.07,\n    \"TFT\": 0.18,\n    \"TCN\": 0.12,",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "NASA_ENSEMBLE_WEIGHTS",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "NASA_ENSEMBLE_WEIGHTS = {\n    \"XGBoost\": 0.30,\n    \"RandomForest\": 0.25,\n    \"LSTM\": 0.08,\n    \"CNN-LSTM\": 0.07,\n    \"TFT\": 0.18,\n    \"TCN\": 0.12,\n}\nNASA_HOUR_CALIBRATION = {\n    0: 0.0,",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "NASA_HOUR_CALIBRATION",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "NASA_HOUR_CALIBRATION = {\n    0: 0.0,\n    1: 0.0,\n    2: 0.0,\n    3: 0.0,\n    4: 0.0,\n    5: 0.0,\n    6: 0.0,\n    7: 1.0,\n    8: 1.0,",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "OPENMETEO_ENSEMBLE_WEIGHTS",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "OPENMETEO_ENSEMBLE_WEIGHTS = {\n    \"XGBoost\": 0.20,\n    \"RandomForest\": 0.18,\n    \"LSTM\": 0.12,\n    \"CNN-LSTM\": 0.10,\n    \"TFT\": 0.25,\n    \"TCN\": 0.15,\n}\nOPENMETEO_HOUR_CALIBRATION = {\n    0: 0.0,",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "OPENMETEO_HOUR_CALIBRATION",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "OPENMETEO_HOUR_CALIBRATION = {\n    0: 0.0,\n    1: 0.0,\n    2: 0.0,\n    3: 0.0,\n    4: 0.0,\n    5: 0.0,\n    6: 0.0,\n    7: 0.0,\n    8: 0.22,",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "TREE_FEATURES",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "TREE_FEATURES = [\n    \"hour\",\n    \"month\",\n    \"day_of_year\",\n    \"hour_sin\",\n    \"hour_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"doy_sin\",\n    \"doy_cos\",",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "SEQUENCE_FEATURES",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "SEQUENCE_FEATURES = [\n    \"hour\",\n    \"month\",\n    \"day_of_year\",\n    \"hour_sin\",\n    \"hour_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"doy_sin\",\n    \"doy_cos\",",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "TARGET_COLUMN",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "TARGET_COLUMN = \"SolarRadiation\"\n# ============== Visualization Colors ==============\nMODEL_COLORS = {\n    \"XGBoost\": \"#e74c3c\",\n    \"Random Forest\": \"#2ecc71\",\n    \"LSTM\": \"#3498db\",\n    \"CNN-LSTM\": \"#9b59b6\",\n    \"TFT\": \"#1abc9c\",\n    \"TCN\": \"#f1c40f\",\n    \"Ensemble\": \"#2c3e50\",",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "MODEL_COLORS",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "MODEL_COLORS = {\n    \"XGBoost\": \"#e74c3c\",\n    \"Random Forest\": \"#2ecc71\",\n    \"LSTM\": \"#3498db\",\n    \"CNN-LSTM\": \"#9b59b6\",\n    \"TFT\": \"#1abc9c\",\n    \"TCN\": \"#f1c40f\",\n    \"Ensemble\": \"#2c3e50\",\n    \"NASA POWER\": \"#e67e22\",\n    \"Open-Meteo\": \"#f39c12\",",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "API_TIMEOUT",
        "kind": 5,
        "importPath": "frontend.config",
        "description": "frontend.config",
        "peekOfCode": "API_TIMEOUT = 30  # seconds",
        "detail": "frontend.config",
        "documentation": {}
    },
    {
        "label": "load_historical_data",
        "kind": 2,
        "importPath": "frontend.data",
        "description": "frontend.data",
        "peekOfCode": "def load_historical_data() -> pd.DataFrame:\n    \"\"\"\n    Load and preprocess historical solar radiation data.\n    Returns:\n        Preprocessed DataFrame with all features\n    \"\"\"\n    df = pd.read_csv(\n        DATA_PATH,\n        parse_dates=[\"datetime\"],\n        dayfirst=True,",
        "detail": "frontend.data",
        "documentation": {}
    },
    {
        "label": "get_data_summary",
        "kind": 2,
        "importPath": "frontend.data",
        "description": "frontend.data",
        "peekOfCode": "def get_data_summary(df: pd.DataFrame) -> dict:\n    \"\"\"\n    Get summary statistics about the data.\n    Args:\n        df: Loaded DataFrame\n    Returns:\n        Dictionary with summary statistics\n    \"\"\"\n    return {\n        \"start_date\": df.index.min().strftime(\"%Y-%m-%d\"),",
        "detail": "frontend.data",
        "documentation": {}
    },
    {
        "label": "load_traditional_models",
        "kind": 2,
        "importPath": "frontend.models",
        "description": "frontend.models",
        "peekOfCode": "def load_traditional_models(model_folder: str) -> Dict[str, Any]:\n    \"\"\"\n    Load traditional ML models (XGBoost, Random Forest).\n    Args:\n        model_folder: Path to folder containing model files\n    Returns:\n        Dictionary with loaded models\n    \"\"\"\n    models = {}\n    # Load XGBoost",
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "load_keras_models",
        "kind": 2,
        "importPath": "frontend.models",
        "description": "frontend.models",
        "peekOfCode": "def load_keras_models(model_folder: str) -> Dict[str, Any]:\n    \"\"\"\n    Load Keras/TensorFlow models (LSTM, CNN-LSTM).\n    Args:\n        model_folder: Path to folder containing model files\n    Returns:\n        Dictionary with loaded models\n    \"\"\"\n    models = {}\n    if not TENSORFLOW_AVAILABLE:",
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "load_pytorch_models",
        "kind": 2,
        "importPath": "frontend.models",
        "description": "frontend.models",
        "peekOfCode": "def load_pytorch_models(model_folder: str, input_size: int = 17) -> Dict[str, Any]:\n    \"\"\"\n    Load PyTorch models (TFT, TCN).\n    Args:\n        model_folder: Path to folder containing model files\n        input_size: Number of input features (default: 17)\n    Returns:\n        Dictionary with loaded models\n    \"\"\"\n    models = {}",
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "load_scalers",
        "kind": 2,
        "importPath": "frontend.models",
        "description": "frontend.models",
        "peekOfCode": "def load_scalers(model_folder: str) -> Tuple[Optional[Any], Optional[Any]]:\n    \"\"\"\n    Load feature and target scalers.\n    Args:\n        model_folder: Path to folder containing scaler files\n    Returns:\n        Tuple of (scaler_X, scaler_y) or (None, None) if not found\n    \"\"\"\n    scaler_X, scaler_y = None, None\n    try:",
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "load_ensemble_weights",
        "kind": 2,
        "importPath": "frontend.models",
        "description": "frontend.models",
        "peekOfCode": "def load_ensemble_weights(model_folder: str) -> Dict[str, float]:\n    \"\"\"\n    Load ensemble weights from file or return defaults.\n    Args:\n        model_folder: Path to folder containing weights file\n    Returns:\n        Dictionary of model weights\n    \"\"\"\n    default_weights = {\n        \"XGBoost\": 0.20,",
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "load_all_models",
        "kind": 2,
        "importPath": "frontend.models",
        "description": "frontend.models",
        "peekOfCode": "def load_all_models(\n    traditional_folder: str, lstm_folder: str, tft_folder: str, selected_models: list\n) -> Dict[str, Any]:\n    \"\"\"\n    Load all required models based on selection.\n    Args:\n        traditional_folder: Path to traditional models\n        lstm_folder: Path to LSTM models\n        tft_folder: Path to TFT/TCN models\n        selected_models: List of selected model names",
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]",
        "kind": 5,
        "importPath": "frontend.models",
        "description": "frontend.models",
        "peekOfCode": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n# ============== Check Available Frameworks ==============\nTENSORFLOW_AVAILABLE = False\nPYTORCH_AVAILABLE = False\nXGBOOST_AVAILABLE = False\ntry:\n    import tensorflow as tf\n    # Use tf.keras for Keras 3.x compatibility with Keras 2.x saved models\n    # This handles the time_major parameter issue\n    keras_load_model = tf.keras.models.load_model",
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "TENSORFLOW_AVAILABLE",
        "kind": 5,
        "importPath": "frontend.models",
        "description": "frontend.models",
        "peekOfCode": "TENSORFLOW_AVAILABLE = False\nPYTORCH_AVAILABLE = False\nXGBOOST_AVAILABLE = False\ntry:\n    import tensorflow as tf\n    # Use tf.keras for Keras 3.x compatibility with Keras 2.x saved models\n    # This handles the time_major parameter issue\n    keras_load_model = tf.keras.models.load_model\n    TENSORFLOW_AVAILABLE = True\nexcept ImportError:",
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "PYTORCH_AVAILABLE",
        "kind": 5,
        "importPath": "frontend.models",
        "description": "frontend.models",
        "peekOfCode": "PYTORCH_AVAILABLE = False\nXGBOOST_AVAILABLE = False\ntry:\n    import tensorflow as tf\n    # Use tf.keras for Keras 3.x compatibility with Keras 2.x saved models\n    # This handles the time_major parameter issue\n    keras_load_model = tf.keras.models.load_model\n    TENSORFLOW_AVAILABLE = True\nexcept ImportError:\n    pass",
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "XGBOOST_AVAILABLE",
        "kind": 5,
        "importPath": "frontend.models",
        "description": "frontend.models",
        "peekOfCode": "XGBOOST_AVAILABLE = False\ntry:\n    import tensorflow as tf\n    # Use tf.keras for Keras 3.x compatibility with Keras 2.x saved models\n    # This handles the time_major parameter issue\n    keras_load_model = tf.keras.models.load_model\n    TENSORFLOW_AVAILABLE = True\nexcept ImportError:\n    pass\ntry:",
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "frontend.models",
        "description": "frontend.models",
        "peekOfCode": "__all__ = [\n    \"TENSORFLOW_AVAILABLE\",\n    \"PYTORCH_AVAILABLE\",\n    \"XGBOOST_AVAILABLE\",\n    \"load_traditional_models\",\n    \"load_keras_models\",\n    \"load_pytorch_models\",\n    \"load_scalers\",\n    \"load_ensemble_weights\",\n    \"load_all_models\",",
        "detail": "frontend.models",
        "documentation": {}
    },
    {
        "label": "PredictionEngine",
        "kind": 6,
        "importPath": "frontend.prediction",
        "description": "frontend.prediction",
        "peekOfCode": "class PredictionEngine:\n    \"\"\"\n    Unified prediction engine for all model types.\n    \"\"\"\n    def __init__(\n        self, models: Dict[str, Any], scaler_X: Any, scaler_y: Any, df_day: pd.DataFrame\n    ):\n        \"\"\"\n        Initialize prediction engine.\n        Args:",
        "detail": "frontend.prediction",
        "documentation": {}
    },
    {
        "label": "create_forecast_plots",
        "kind": 2,
        "importPath": "frontend.visualization",
        "description": "frontend.visualization",
        "peekOfCode": "def create_forecast_plots(\n    results_df: pd.DataFrame,\n    selected_models: List[str],\n    api_preds: Optional[List[float]],\n    api_name: str,\n    ensemble_preds: List[float],\n    forecast_date: str,\n) -> plt.Figure:\n    \"\"\"\n    Create comprehensive forecast visualization.",
        "detail": "frontend.visualization",
        "documentation": {}
    },
    {
        "label": "create_deep_learning_comparison_plot",
        "kind": 2,
        "importPath": "frontend.visualization",
        "description": "frontend.visualization",
        "peekOfCode": "def create_deep_learning_comparison_plot(\n    results_df: pd.DataFrame, api_preds: Optional[List[float]], api_name: str\n) -> plt.Figure:\n    \"\"\"\n    Create comparison plot specifically for deep learning models.\n    Args:\n        results_df: DataFrame with predictions\n        api_preds: API predictions\n        api_name: Name of API used\n    Returns:",
        "detail": "frontend.visualization",
        "documentation": {}
    },
    {
        "label": "calculate_metrics",
        "kind": 2,
        "importPath": "frontend.visualization",
        "description": "frontend.visualization",
        "peekOfCode": "def calculate_metrics(\n    predictions: Dict[str, List[float]], api_preds: List[float]\n) -> pd.DataFrame:\n    \"\"\"\n    Calculate comparison metrics between models and API.\n    Args:\n        predictions: Dictionary of model predictions\n        api_preds: API predictions (ground truth)\n    Returns:\n        DataFrame with metrics for each model",
        "detail": "frontend.visualization",
        "documentation": {}
    },
    {
        "label": "create_model_architecture_info",
        "kind": 2,
        "importPath": "frontend.visualization",
        "description": "frontend.visualization",
        "peekOfCode": "def create_model_architecture_info() -> Dict[str, Dict[str, str]]:\n    \"\"\"\n    Return information about each model architecture.\n    Returns:\n        Dictionary with model information\n    \"\"\"\n    return {\n        \"XGBoost\": {\n            \"type\": \"Tree-Based Ensemble\",\n            \"description\": \"Gradient boosting with decision trees\",",
        "detail": "frontend.visualization",
        "documentation": {}
    },
    {
        "label": "cache_session",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\nretry_session = retry(cache_session, retries=5, backoff_factor=0.2)\nopenmeteo = openmeteo_requests.Client(session=retry_session)\n# Make sure all required weather variables are listed here\nurl = \"https://historical-forecast-api.open-meteo.com/v1/forecast\"\nparams = {\n    \"latitude\": 33.7215,\n    \"longitude\": 73.0433,\n    \"start_date\": \"2016-01-01\",\n    \"end_date\": \"2025-10-28\",",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "retry_session",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\nopenmeteo = openmeteo_requests.Client(session=retry_session)\n# Make sure all required weather variables are listed here\nurl = \"https://historical-forecast-api.open-meteo.com/v1/forecast\"\nparams = {\n    \"latitude\": 33.7215,\n    \"longitude\": 73.0433,\n    \"start_date\": \"2016-01-01\",\n    \"end_date\": \"2025-10-28\",\n    \"daily\": [\"sunset\", \"uv_index_max\", \"apparent_temperature_max\", \"snowfall_sum\"],",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "openmeteo",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "openmeteo = openmeteo_requests.Client(session=retry_session)\n# Make sure all required weather variables are listed here\nurl = \"https://historical-forecast-api.open-meteo.com/v1/forecast\"\nparams = {\n    \"latitude\": 33.7215,\n    \"longitude\": 73.0433,\n    \"start_date\": \"2016-01-01\",\n    \"end_date\": \"2025-10-28\",\n    \"daily\": [\"sunset\", \"uv_index_max\", \"apparent_temperature_max\", \"snowfall_sum\"],\n    \"hourly\": [\"temperature_2m\", \"uv_index_clear_sky\", \"direct_radiation\", \"direct_normal_irradiance\"],",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "url",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "url = \"https://historical-forecast-api.open-meteo.com/v1/forecast\"\nparams = {\n    \"latitude\": 33.7215,\n    \"longitude\": 73.0433,\n    \"start_date\": \"2016-01-01\",\n    \"end_date\": \"2025-10-28\",\n    \"daily\": [\"sunset\", \"uv_index_max\", \"apparent_temperature_max\", \"snowfall_sum\"],\n    \"hourly\": [\"temperature_2m\", \"uv_index_clear_sky\", \"direct_radiation\", \"direct_normal_irradiance\"],\n    \"timezone\": \"Asia/Bangkok\",\n}",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "params",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "params = {\n    \"latitude\": 33.7215,\n    \"longitude\": 73.0433,\n    \"start_date\": \"2016-01-01\",\n    \"end_date\": \"2025-10-28\",\n    \"daily\": [\"sunset\", \"uv_index_max\", \"apparent_temperature_max\", \"snowfall_sum\"],\n    \"hourly\": [\"temperature_2m\", \"uv_index_clear_sky\", \"direct_radiation\", \"direct_normal_irradiance\"],\n    \"timezone\": \"Asia/Bangkok\",\n}\n# API request",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "responses",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "responses = openmeteo.weather_api(url, params=params)\nresponse = responses[0]\nprint(f\"Coordinates: {response.Latitude()}Â°N {response.Longitude()}Â°E\")\nprint(f\"Elevation: {response.Elevation()} m asl\")\nprint(f\"Timezone: {response.Timezone()}{response.TimezoneAbbreviation()}\")\nprint(f\"Timezone difference to GMT+0: {response.UtcOffsetSeconds()}s\")\n# Process hourly data\nhourly = response.Hourly()\nhourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\nhourly_uv_index_clear_sky = hourly.Variables(1).ValuesAsNumpy()",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "response = responses[0]\nprint(f\"Coordinates: {response.Latitude()}Â°N {response.Longitude()}Â°E\")\nprint(f\"Elevation: {response.Elevation()} m asl\")\nprint(f\"Timezone: {response.Timezone()}{response.TimezoneAbbreviation()}\")\nprint(f\"Timezone difference to GMT+0: {response.UtcOffsetSeconds()}s\")\n# Process hourly data\nhourly = response.Hourly()\nhourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\nhourly_uv_index_clear_sky = hourly.Variables(1).ValuesAsNumpy()\nhourly_direct_radiation = hourly.Variables(2).ValuesAsNumpy()",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly = response.Hourly()\nhourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\nhourly_uv_index_clear_sky = hourly.Variables(1).ValuesAsNumpy()\nhourly_direct_radiation = hourly.Variables(2).ValuesAsNumpy()\nhourly_direct_normal_irradiance = hourly.Variables(3).ValuesAsNumpy()\nhourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_temperature_2m",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\nhourly_uv_index_clear_sky = hourly.Variables(1).ValuesAsNumpy()\nhourly_direct_radiation = hourly.Variables(2).ValuesAsNumpy()\nhourly_direct_normal_irradiance = hourly.Variables(3).ValuesAsNumpy()\nhourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),\n        inclusive=\"left\"",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_uv_index_clear_sky",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_uv_index_clear_sky = hourly.Variables(1).ValuesAsNumpy()\nhourly_direct_radiation = hourly.Variables(2).ValuesAsNumpy()\nhourly_direct_normal_irradiance = hourly.Variables(3).ValuesAsNumpy()\nhourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),\n        inclusive=\"left\"\n    ),",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_direct_radiation",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_direct_radiation = hourly.Variables(2).ValuesAsNumpy()\nhourly_direct_normal_irradiance = hourly.Variables(3).ValuesAsNumpy()\nhourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),\n        inclusive=\"left\"\n    ),\n    \"temperature_2m\": hourly_temperature_2m,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_direct_normal_irradiance",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_direct_normal_irradiance = hourly.Variables(3).ValuesAsNumpy()\nhourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),\n        inclusive=\"left\"\n    ),\n    \"temperature_2m\": hourly_temperature_2m,\n    \"uv_index_clear_sky\": hourly_uv_index_clear_sky,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_data",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),\n        inclusive=\"left\"\n    ),\n    \"temperature_2m\": hourly_temperature_2m,\n    \"uv_index_clear_sky\": hourly_uv_index_clear_sky,\n    \"direct_radiation\": hourly_direct_radiation,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_dataframe",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_dataframe = pd.DataFrame(data=hourly_data)\nprint(\"\\nHourly data\\n\", hourly_dataframe.head())\n# Process daily data\ndaily = response.Daily()\ndaily_sunset = daily.Variables(0).ValuesInt64AsNumpy()\ndaily_uv_index_max = daily.Variables(1).ValuesAsNumpy()\ndaily_apparent_temperature_max = daily.Variables(2).ValuesAsNumpy()\ndaily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily = response.Daily()\ndaily_sunset = daily.Variables(0).ValuesInt64AsNumpy()\ndaily_uv_index_max = daily.Variables(1).ValuesAsNumpy()\ndaily_apparent_temperature_max = daily.Variables(2).ValuesAsNumpy()\ndaily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_sunset",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_sunset = daily.Variables(0).ValuesInt64AsNumpy()\ndaily_uv_index_max = daily.Variables(1).ValuesAsNumpy()\ndaily_apparent_temperature_max = daily.Variables(2).ValuesAsNumpy()\ndaily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),\n        inclusive=\"left\"",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_uv_index_max",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_uv_index_max = daily.Variables(1).ValuesAsNumpy()\ndaily_apparent_temperature_max = daily.Variables(2).ValuesAsNumpy()\ndaily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),\n        inclusive=\"left\"\n    ),",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_apparent_temperature_max",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_apparent_temperature_max = daily.Variables(2).ValuesAsNumpy()\ndaily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),\n        inclusive=\"left\"\n    ),\n    \"sunset\": daily_sunset,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_snowfall_sum",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),\n        inclusive=\"left\"\n    ),\n    \"sunset\": daily_sunset,\n    \"uv_index_max\": daily_uv_index_max,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_data",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),\n        inclusive=\"left\"\n    ),\n    \"sunset\": daily_sunset,\n    \"uv_index_max\": daily_uv_index_max,\n    \"apparent_temperature_max\": daily_apparent_temperature_max,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_dataframe",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_dataframe = pd.DataFrame(data=daily_data)\nprint(\"\\nDaily data\\n\", daily_dataframe.head())\n# ðŸ’¾ Save data to CSV files\nhourly_dataframe.to_csv(\"hourly_weather_data.csv\", index=False)\ndaily_dataframe.to_csv(\"daily_weather_data.csv\", index=False)\nprint(\"\\nData saved successfully:\")\nprint(\" - hourly_weather_data.csv\")\nprint(\" - daily_weather_data.csv\")",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "SolarSequenceDataset",
        "kind": 6,
        "importPath": "tft.main",
        "description": "tft.main",
        "peekOfCode": "class SolarSequenceDataset(Dataset):\n    \"\"\"\n    Creates sequences from scaled data for TFT training.\n    Matches the sequence creation logic in training.py.\n    \"\"\"\n    def __init__(self, X_scaled, y_scaled, seq_len=24):\n        self.X = X_scaled\n        self.y = y_scaled\n        self.seq_len = seq_len\n    def __len__(self):",
        "detail": "tft.main",
        "documentation": {}
    },
    {
        "label": "rmse",
        "kind": 2,
        "importPath": "tft.main",
        "description": "tft.main",
        "peekOfCode": "def rmse(a, b):\n    return np.sqrt(mean_squared_error(a, b))\n# ---------------------------------------------------\n# Custom Dataset for TFT (Sequence-based)\n# ---------------------------------------------------\nclass SolarSequenceDataset(Dataset):\n    \"\"\"\n    Creates sequences from scaled data for TFT training.\n    Matches the sequence creation logic in training.py.\n    \"\"\"",
        "detail": "tft.main",
        "documentation": {}
    },
    {
        "label": "train_tft",
        "kind": 2,
        "importPath": "tft.main",
        "description": "tft.main",
        "peekOfCode": "def train_tft():\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ðŸŒ¤ TFT (Temporal Fusion Transformer) Training\")\n    print(\"=\" * 60)\n    # ---------- 1. Load data (same as training.py) ----------\n    data_path = (\n        \"../NASA meteriological and solar radiaton data/lahore_hourly_filled.csv\"\n    )\n    df = pd.read_csv(data_path)\n    df.columns = df.columns.str.strip()",
        "detail": "tft.main",
        "documentation": {}
    },
    {
        "label": "RANDOM_STATE",
        "kind": 5,
        "importPath": "tft.main",
        "description": "tft.main",
        "peekOfCode": "RANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntorch.manual_seed(RANDOM_STATE)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(RANDOM_STATE)\n# Create save directory (same as training.py)\nos.makedirs(\"../saved_models\", exist_ok=True)\ndef rmse(a, b):\n    return np.sqrt(mean_squared_error(a, b))\n# ---------------------------------------------------",
        "detail": "tft.main",
        "documentation": {}
    },
    {
        "label": "CausalConv1d",
        "kind": 6,
        "importPath": "tft.tcn",
        "description": "tft.tcn",
        "peekOfCode": "class CausalConv1d(nn.Module):\n    \"\"\"\n    Causal convolution layer that ensures no future information leakage.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n        super().__init__()\n        self.padding = (kernel_size - 1) * dilation\n        self.conv = nn.Conv1d(\n            in_channels,\n            out_channels,",
        "detail": "tft.tcn",
        "documentation": {}
    },
    {
        "label": "TemporalBlock",
        "kind": 6,
        "importPath": "tft.tcn",
        "description": "tft.tcn",
        "peekOfCode": "class TemporalBlock(nn.Module):\n    \"\"\"\n    A residual block with causal dilated convolutions.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2):\n        super().__init__()\n        # First causal convolution\n        self.conv1 = CausalConv1d(in_channels, out_channels, kernel_size, dilation)\n        self.bn1 = nn.BatchNorm1d(out_channels)\n        self.relu1 = nn.ReLU()",
        "detail": "tft.tcn",
        "documentation": {}
    },
    {
        "label": "TemporalConvolutionalNetwork",
        "kind": 6,
        "importPath": "tft.tcn",
        "description": "tft.tcn",
        "peekOfCode": "class TemporalConvolutionalNetwork(nn.Module):\n    \"\"\"\n    TCN architecture for time series forecasting.\n    Uses dilated causal convolutions to capture long-range dependencies.\n    \"\"\"\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        output_size,",
        "detail": "tft.tcn",
        "documentation": {}
    },
    {
        "label": "LitTCN",
        "kind": 6,
        "importPath": "tft.tcn",
        "description": "tft.tcn",
        "peekOfCode": "class LitTCN(L.LightningModule):\n    \"\"\"\n    PyTorch Lightning wrapper for TCN model.\n    Aligned with training.py metrics and evaluation structure.\n    \"\"\"\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        output_size,",
        "detail": "tft.tcn",
        "documentation": {}
    },
    {
        "label": "TemporalFusionTransformer",
        "kind": 6,
        "importPath": "tft.tft",
        "description": "tft.tft",
        "peekOfCode": "class TemporalFusionTransformer(nn.Module):\n    \"\"\"\n    Simplified TFT architecture with multi-head attention.\n    Aligned with the training.py workflow for solar radiation prediction.\n    \"\"\"\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        output_size,",
        "detail": "tft.tft",
        "documentation": {}
    },
    {
        "label": "LitTFT",
        "kind": 6,
        "importPath": "tft.tft",
        "description": "tft.tft",
        "peekOfCode": "class LitTFT(L.LightningModule):\n    \"\"\"\n    PyTorch Lightning wrapper for TFT model.\n    Aligned with training.py metrics and evaluation structure.\n    \"\"\"\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        output_size,",
        "detail": "tft.tft",
        "documentation": {}
    },
    {
        "label": "rmse",
        "kind": 2,
        "importPath": "tft.train",
        "description": "tft.train",
        "peekOfCode": "def rmse(a, b):\n    return np.sqrt(mean_squared_error(a, b))\n# ---------------------------------------------------\n# Data Loading and Preprocessing\n# ---------------------------------------------------\ndef load_and_prepare_data():\n    \"\"\"\n    Load data and prepare features exactly as training.py does.\n    Returns prepared data loaders and scalers.\n    \"\"\"",
        "detail": "tft.train",
        "documentation": {}
    },
    {
        "label": "load_and_prepare_data",
        "kind": 2,
        "importPath": "tft.train",
        "description": "tft.train",
        "peekOfCode": "def load_and_prepare_data():\n    \"\"\"\n    Load data and prepare features exactly as training.py does.\n    Returns prepared data loaders and scalers.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ðŸ“Š Loading and Preparing Data\")\n    print(\"=\" * 60)\n    # Load data (same as training.py)\n    data_path = (",
        "detail": "tft.train",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": "tft.train",
        "description": "tft.train",
        "peekOfCode": "def train_model(model_name, data, max_epochs=100, learning_rate=1e-3):\n    \"\"\"\n    Train a model (TFT or TCN) and return metrics.\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"ðŸš€ Training {model_name}\")\n    print(f\"{'='*60}\")\n    input_size = data[\"input_size\"]\n    hidden_size = 64\n    output_size = 1",
        "detail": "tft.train",
        "documentation": {}
    },
    {
        "label": "update_results",
        "kind": 2,
        "importPath": "tft.train",
        "description": "tft.train",
        "peekOfCode": "def update_results(metrics_list):\n    \"\"\"\n    Update the model_results.csv with new metrics.\n    \"\"\"\n    results_path = \"../saved_models/model_results.csv\"\n    try:\n        results_df = pd.read_csv(results_path)\n    except FileNotFoundError:\n        results_df = pd.DataFrame(\n            columns=[\"Model\", \"R2_Train\", \"R2_Test\", \"MAE_Test\", \"RMSE_Test\"]",
        "detail": "tft.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tft.train",
        "description": "tft.train",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(\n        description=\"Train TFT and/or TCN models for Solar Radiation Prediction\"\n    )\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        choices=[\"tft\", \"tcn\", \"all\"],\n        default=\"all\",\n        help=\"Model to train: 'tft', 'tcn', or 'all' (default: all)\",",
        "detail": "tft.train",
        "documentation": {}
    },
    {
        "label": "RANDOM_STATE",
        "kind": 5,
        "importPath": "tft.train",
        "description": "tft.train",
        "peekOfCode": "RANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntorch.manual_seed(RANDOM_STATE)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(RANDOM_STATE)\n# Create save directory (same as training.py)\nos.makedirs(\"../saved_models\", exist_ok=True)\ndef rmse(a, b):\n    return np.sqrt(mean_squared_error(a, b))\n# ---------------------------------------------------",
        "detail": "tft.train",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nprint(\"Converting LSTM model...\")\n# Recreate LSTM architecture\nlstm_model = models.Sequential(\n    [\n        layers.Input(shape=(24, 17)),\n        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n        layers.Dropout(0.2),",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "lstm_model",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "lstm_model = models.Sequential(\n    [\n        layers.Input(shape=(24, 17)),\n        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n        layers.Dropout(0.2),\n        layers.Bidirectional(layers.LSTM(32)),\n        layers.Dropout(0.2),\n        layers.Dense(32, activation=\"relu\"),\n        layers.Dense(16, activation=\"relu\"),\n        layers.Dense(1),",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "inp = layers.Input(shape=(24, 17), name=\"input_1\")\nx = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1d\")(inp)\nx = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1d_1\")(x)\nx = layers.BatchNormalization(name=\"batch_normalization\")(x)\nx = layers.MaxPool1D(2, name=\"max_pooling1d\")(x)\nx = layers.Dropout(0.2, name=\"dropout\")(x)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "x = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1d\")(inp)\nx = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1d_1\")(x)\nx = layers.BatchNormalization(name=\"batch_normalization\")(x)\nx = layers.MaxPool1D(2, name=\"max_pooling1d\")(x)\nx = layers.Dropout(0.2, name=\"dropout\")(x)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "x = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1d_1\")(x)\nx = layers.BatchNormalization(name=\"batch_normalization\")(x)\nx = layers.MaxPool1D(2, name=\"max_pooling1d\")(x)\nx = layers.Dropout(0.2, name=\"dropout\")(x)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "x = layers.BatchNormalization(name=\"batch_normalization\")(x)\nx = layers.MaxPool1D(2, name=\"max_pooling1d\")(x)\nx = layers.Dropout(0.2, name=\"dropout\")(x)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\n# Match saved model: dense expects input 64 -> output 32",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "x = layers.MaxPool1D(2, name=\"max_pooling1d\")(x)\nx = layers.Dropout(0.2, name=\"dropout\")(x)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\n# Match saved model: dense expects input 64 -> output 32\nx = layers.Dense(32, activation=\"relu\", name=\"dense\")(x)",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "x = layers.Dropout(0.2, name=\"dropout\")(x)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\n# Match saved model: dense expects input 64 -> output 32\nx = layers.Dense(32, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(16, activation=\"relu\", name=\"dense_1\")(x)",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\n# Match saved model: dense expects input 64 -> output 32\nx = layers.Dense(32, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(16, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "x = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\n# Match saved model: dense expects input 64 -> output 32\nx = layers.Dense(32, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(16, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\n# Load weights by name\ncnn_lstm_model.load_weights(\"saved_models/cnn_lstm_model.h5\", by_name=True)",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "x = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\n# Match saved model: dense expects input 64 -> output 32\nx = layers.Dense(32, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(16, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\n# Load weights by name\ncnn_lstm_model.load_weights(\"saved_models/cnn_lstm_model.h5\", by_name=True)\ncnn_lstm_model.save(\"saved_models/cnn_lstm_model_v3.keras\")",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "x = layers.Dropout(0.2, name=\"dropout_2\")(x)\n# Match saved model: dense expects input 64 -> output 32\nx = layers.Dense(32, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(16, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\n# Load weights by name\ncnn_lstm_model.load_weights(\"saved_models/cnn_lstm_model.h5\", by_name=True)\ncnn_lstm_model.save(\"saved_models/cnn_lstm_model_v3.keras\")\nprint(\"CNN-LSTM model saved as cnn_lstm_model_v3.keras\")",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "x = layers.Dense(32, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(16, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\n# Load weights by name\ncnn_lstm_model.load_weights(\"saved_models/cnn_lstm_model.h5\", by_name=True)\ncnn_lstm_model.save(\"saved_models/cnn_lstm_model_v3.keras\")\nprint(\"CNN-LSTM model saved as cnn_lstm_model_v3.keras\")\nprint(\"\\nDone! Models converted to Keras 3.x format.\")",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "x = layers.Dense(16, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\n# Load weights by name\ncnn_lstm_model.load_weights(\"saved_models/cnn_lstm_model.h5\", by_name=True)\ncnn_lstm_model.save(\"saved_models/cnn_lstm_model_v3.keras\")\nprint(\"CNN-LSTM model saved as cnn_lstm_model_v3.keras\")\nprint(\"\\nDone! Models converted to Keras 3.x format.\")",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "out",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "out = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\n# Load weights by name\ncnn_lstm_model.load_weights(\"saved_models/cnn_lstm_model.h5\", by_name=True)\ncnn_lstm_model.save(\"saved_models/cnn_lstm_model_v3.keras\")\nprint(\"CNN-LSTM model saved as cnn_lstm_model_v3.keras\")\nprint(\"\\nDone! Models converted to Keras 3.x format.\")",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "cnn_lstm_model",
        "kind": 5,
        "importPath": "convert_models",
        "description": "convert_models",
        "peekOfCode": "cnn_lstm_model = models.Model(inp, out)\n# Load weights by name\ncnn_lstm_model.load_weights(\"saved_models/cnn_lstm_model.h5\", by_name=True)\ncnn_lstm_model.save(\"saved_models/cnn_lstm_model_v3.keras\")\nprint(\"CNN-LSTM model saved as cnn_lstm_model_v3.keras\")\nprint(\"\\nDone! Models converted to Keras 3.x format.\")",
        "detail": "convert_models",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nMODEL_FOLDER = \"saved_models_lstm\"\nprint(\"Converting LSTM model from saved_models_lstm...\")\n# Recreate LSTM architecture (matches the saved model structure)\nlstm_model = models.Sequential(\n    [\n        layers.Input(shape=(24, 15)),  # 15 features based on saved_models_lstm\n        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "MODEL_FOLDER",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "MODEL_FOLDER = \"saved_models_lstm\"\nprint(\"Converting LSTM model from saved_models_lstm...\")\n# Recreate LSTM architecture (matches the saved model structure)\nlstm_model = models.Sequential(\n    [\n        layers.Input(shape=(24, 15)),  # 15 features based on saved_models_lstm\n        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n        layers.Dropout(0.2),\n        layers.Bidirectional(layers.LSTM(32)),\n        layers.Dropout(0.2),",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "lstm_model",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "lstm_model = models.Sequential(\n    [\n        layers.Input(shape=(24, 15)),  # 15 features based on saved_models_lstm\n        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n        layers.Dropout(0.2),\n        layers.Bidirectional(layers.LSTM(32)),\n        layers.Dropout(0.2),\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(32, activation=\"relu\"),\n        layers.Dense(1),",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "inp = layers.Input(shape=(24, 15), name=\"input_layer\")\nx = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1d\")(inp)\nx = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1d_1\")(x)\nx = layers.MaxPool1D(2, name=\"max_pooling1d\")(x)\nx = layers.Dropout(0.2, name=\"dropout\")(x)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "x = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1d\")(inp)\nx = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1d_1\")(x)\nx = layers.MaxPool1D(2, name=\"max_pooling1d\")(x)\nx = layers.Dropout(0.2, name=\"dropout\")(x)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "x = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1d_1\")(x)\nx = layers.MaxPool1D(2, name=\"max_pooling1d\")(x)\nx = layers.Dropout(0.2, name=\"dropout\")(x)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\nx = layers.Dense(64, activation=\"relu\", name=\"dense\")(x)",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "x = layers.MaxPool1D(2, name=\"max_pooling1d\")(x)\nx = layers.Dropout(0.2, name=\"dropout\")(x)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\nx = layers.Dense(64, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(32, activation=\"relu\", name=\"dense_1\")(x)",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "x = layers.Dropout(0.2, name=\"dropout\")(x)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\nx = layers.Dense(64, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(32, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name=\"bidirectional\")(\n    x\n)\nx = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\nx = layers.Dense(64, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(32, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "x = layers.Dropout(0.2, name=\"dropout_1\")(x)\nx = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\nx = layers.Dense(64, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(32, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\ntry:\n    cnn_lstm_model.load_weights(f\"{MODEL_FOLDER}/cnn_lstm_model.h5\", by_name=True)\n    cnn_lstm_model.save(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\")",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "x = layers.Bidirectional(layers.LSTM(32), name=\"bidirectional_1\")(x)\nx = layers.Dropout(0.2, name=\"dropout_2\")(x)\nx = layers.Dense(64, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(32, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\ntry:\n    cnn_lstm_model.load_weights(f\"{MODEL_FOLDER}/cnn_lstm_model.h5\", by_name=True)\n    cnn_lstm_model.save(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\")\n    print(\"CNN-LSTM model saved as cnn_lstm_model_v3.keras\")",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "x = layers.Dropout(0.2, name=\"dropout_2\")(x)\nx = layers.Dense(64, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(32, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\ntry:\n    cnn_lstm_model.load_weights(f\"{MODEL_FOLDER}/cnn_lstm_model.h5\", by_name=True)\n    cnn_lstm_model.save(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\")\n    print(\"CNN-LSTM model saved as cnn_lstm_model_v3.keras\")\nexcept Exception as e:",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "x = layers.Dense(64, activation=\"relu\", name=\"dense\")(x)\nx = layers.Dense(32, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\ntry:\n    cnn_lstm_model.load_weights(f\"{MODEL_FOLDER}/cnn_lstm_model.h5\", by_name=True)\n    cnn_lstm_model.save(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\")\n    print(\"CNN-LSTM model saved as cnn_lstm_model_v3.keras\")\nexcept Exception as e:\n    print(f\"CNN-LSTM load failed with 15 features, trying 17: {e}\")",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "x = layers.Dense(32, activation=\"relu\", name=\"dense_1\")(x)\nout = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\ntry:\n    cnn_lstm_model.load_weights(f\"{MODEL_FOLDER}/cnn_lstm_model.h5\", by_name=True)\n    cnn_lstm_model.save(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\")\n    print(\"CNN-LSTM model saved as cnn_lstm_model_v3.keras\")\nexcept Exception as e:\n    print(f\"CNN-LSTM load failed with 15 features, trying 17: {e}\")\n    # Try with 17 features",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "out",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "out = layers.Dense(1, name=\"dense_2\")(x)\ncnn_lstm_model = models.Model(inp, out)\ntry:\n    cnn_lstm_model.load_weights(f\"{MODEL_FOLDER}/cnn_lstm_model.h5\", by_name=True)\n    cnn_lstm_model.save(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\")\n    print(\"CNN-LSTM model saved as cnn_lstm_model_v3.keras\")\nexcept Exception as e:\n    print(f\"CNN-LSTM load failed with 15 features, trying 17: {e}\")\n    # Try with 17 features\n    inp = layers.Input(shape=(24, 17), name=\"input_layer\")",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "cnn_lstm_model",
        "kind": 5,
        "importPath": "convert_models_lstm",
        "description": "convert_models_lstm",
        "peekOfCode": "cnn_lstm_model = models.Model(inp, out)\ntry:\n    cnn_lstm_model.load_weights(f\"{MODEL_FOLDER}/cnn_lstm_model.h5\", by_name=True)\n    cnn_lstm_model.save(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\")\n    print(\"CNN-LSTM model saved as cnn_lstm_model_v3.keras\")\nexcept Exception as e:\n    print(f\"CNN-LSTM load failed with 15 features, trying 17: {e}\")\n    # Try with 17 features\n    inp = layers.Input(shape=(24, 17), name=\"input_layer\")\n    x = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1d\")(inp)",
        "detail": "convert_models_lstm",
        "documentation": {}
    },
    {
        "label": "fetch_openmeteo_solar_forecast",
        "kind": 2,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "def fetch_openmeteo_solar_forecast(year, month, day):\n    \"\"\"Fetch solar radiation forecast from Open-Meteo API.\"\"\"\n    date_str = f\"{year}-{month:02d}-{day:02d}\"\n    url = \"https://api.open-meteo.com/v1/forecast\"\n    params = {\n        \"latitude\": LATITUDE,\n        \"longitude\": LONGITUDE,\n        \"hourly\": \"shortwave_radiation\",\n        \"start_date\": date_str,\n        \"end_date\": date_str,",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "fetch_nasa_power_solar",
        "kind": 2,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "def fetch_nasa_power_solar(year, month, day):\n    \"\"\"\n    Fetch solar radiation data from NASA POWER API.\n    Returns hourly ALLSKY_SFC_SW_DWN (Surface Shortwave Downward Irradiance) in W/mÂ².\n    Note: Historical data only (~1 week delay)\n    \"\"\"\n    date_str = f\"{year}{month:02d}{day:02d}\"\n    url = \"https://power.larc.nasa.gov/api/temporal/hourly/point\"\n    params = {\n        \"parameters\": \"ALLSKY_SFC_SW_DWN\",",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\ntry:\n    from tensorflow.keras.models import load_model\n    TENSORFLOW_AVAILABLE = True\nexcept:\n    TENSORFLOW_AVAILABLE = False\ntry:\n    import xgboost as xgb\n    XGBOOST_AVAILABLE = True\nexcept:",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "LATITUDE",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "LATITUDE = 31.56\nLONGITUDE = 74.35\nTIMEZONE = \"Asia/Karachi\"\ndef fetch_openmeteo_solar_forecast(year, month, day):\n    \"\"\"Fetch solar radiation forecast from Open-Meteo API.\"\"\"\n    date_str = f\"{year}-{month:02d}-{day:02d}\"\n    url = \"https://api.open-meteo.com/v1/forecast\"\n    params = {\n        \"latitude\": LATITUDE,\n        \"longitude\": LONGITUDE,",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "LONGITUDE",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "LONGITUDE = 74.35\nTIMEZONE = \"Asia/Karachi\"\ndef fetch_openmeteo_solar_forecast(year, month, day):\n    \"\"\"Fetch solar radiation forecast from Open-Meteo API.\"\"\"\n    date_str = f\"{year}-{month:02d}-{day:02d}\"\n    url = \"https://api.open-meteo.com/v1/forecast\"\n    params = {\n        \"latitude\": LATITUDE,\n        \"longitude\": LONGITUDE,\n        \"hourly\": \"shortwave_radiation\",",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "TIMEZONE",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "TIMEZONE = \"Asia/Karachi\"\ndef fetch_openmeteo_solar_forecast(year, month, day):\n    \"\"\"Fetch solar radiation forecast from Open-Meteo API.\"\"\"\n    date_str = f\"{year}-{month:02d}-{day:02d}\"\n    url = \"https://api.open-meteo.com/v1/forecast\"\n    params = {\n        \"latitude\": LATITUDE,\n        \"longitude\": LONGITUDE,\n        \"hourly\": \"shortwave_radiation\",\n        \"start_date\": date_str,",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "NASA_ENSEMBLE_WEIGHTS",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "NASA_ENSEMBLE_WEIGHTS = {\n    \"XGBoost\": 0.45,\n    \"RandomForest\": 0.40,\n    \"LSTM\": 0.08,\n    \"CNN-LSTM\": 0.07,\n}\nNASA_HOUR_CALIBRATION = {\n    0: 0.0,\n    1: 0.0,\n    2: 0.0,",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "NASA_HOUR_CALIBRATION",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "NASA_HOUR_CALIBRATION = {\n    0: 0.0,\n    1: 0.0,\n    2: 0.0,\n    3: 0.0,\n    4: 0.0,\n    5: 0.0,\n    6: 0.0,\n    7: 1.0,\n    8: 1.0,",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "OPENMETEO_ENSEMBLE_WEIGHTS",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "OPENMETEO_ENSEMBLE_WEIGHTS = {\n    \"XGBoost\": 0.257051842842307,\n    \"RandomForest\": 0.2569732086168074,\n    \"LSTM\": 0.24311723628987492,\n    \"CNN-LSTM\": 0.24285771225101077,\n}\nOPENMETEO_HOUR_CALIBRATION = {\n    0: 0.0,\n    1: 0.0,\n    2: 0.0,",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "OPENMETEO_HOUR_CALIBRATION",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "OPENMETEO_HOUR_CALIBRATION = {\n    0: 0.0,\n    1: 0.0,\n    2: 0.0,\n    3: 0.0,\n    4: 0.0,\n    5: 0.0,\n    6: 0.0,\n    7: 0.0,\n    8: 0.22,",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "selected_date",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "selected_date = st.sidebar.date_input(\n    \"ðŸ“… Forecast Date\",\n    datetime.today(),\n    help=\"Select the date for solar radiation forecast\",\n)\n# Model folder selection\nmodel_folder = st.sidebar.selectbox(\n    \"ðŸ“ Model Folder\",\n    [\"saved_models_lstm\", \"saved_models\", \"save_model\", \"saved_models_1\"],\n    help=\"Select folder containing trained models (saved_models_lstm has best LSTM/CNN-LSTM scores)\",",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "model_folder",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "model_folder = st.sidebar.selectbox(\n    \"ðŸ“ Model Folder\",\n    [\"saved_models_lstm\", \"saved_models\", \"save_model\", \"saved_models_1\"],\n    help=\"Select folder containing trained models (saved_models_lstm has best LSTM/CNN-LSTM scores)\",\n)\n# Model selection\navailable_models = [\"XGBoost\", \"Random Forest\", \"LSTM\", \"CNN-LSTM\", \"Ensemble\"]\nselected_models = st.sidebar.multiselect(\n    \"ðŸ¤– Select Models\",\n    available_models,",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "available_models",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "available_models = [\"XGBoost\", \"Random Forest\", \"LSTM\", \"CNN-LSTM\", \"Ensemble\"]\nselected_models = st.sidebar.multiselect(\n    \"ðŸ¤– Select Models\",\n    available_models,\n    default=[\"XGBoost\", \"Random Forest\", \"Ensemble\"],\n    help=\"Choose which models to use for prediction\",\n)\nst.sidebar.markdown(\"---\")\n# API Comparison toggle\nenable_api = st.sidebar.checkbox(",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "selected_models",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "selected_models = st.sidebar.multiselect(\n    \"ðŸ¤– Select Models\",\n    available_models,\n    default=[\"XGBoost\", \"Random Forest\", \"Ensemble\"],\n    help=\"Choose which models to use for prediction\",\n)\nst.sidebar.markdown(\"---\")\n# API Comparison toggle\nenable_api = st.sidebar.checkbox(\n    \"ðŸŒ Compare with API\",",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "enable_api",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "enable_api = st.sidebar.checkbox(\n    \"ðŸŒ Compare with API\",\n    value=True,\n    help=\"Fetch API forecast for comparison\",\n)\n# API Selection\nselected_api = st.sidebar.selectbox(\n    \"ðŸ“¡ Select API\",\n    [\"NASA POWER\", \"Open-Meteo\"],\n    index=0,",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "selected_api",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "selected_api = st.sidebar.selectbox(\n    \"ðŸ“¡ Select API\",\n    [\"NASA POWER\", \"Open-Meteo\"],\n    index=0,\n    help=\"NASA POWER: Historical data (same source as training, ~1 week delay)\\nOpen-Meteo: Real-time forecast (today + 16 days)\",\n    disabled=not enable_api,\n)\n# Show API info\nif enable_api:\n    if selected_api == \"NASA POWER\":",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "run_frontend",
        "description": "run_frontend",
        "peekOfCode": "def main():\n    \"\"\"Launch the Streamlit dashboard.\"\"\"\n    # Get the directory of this script\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    # Path to the frontend app\n    app_path = os.path.join(script_dir, \"frontend\", \"app.py\")\n    if not os.path.exists(app_path):\n        print(f\"Error: Frontend app not found at {app_path}\")\n        sys.exit(1)\n    print(\"ðŸŒ¤ Starting Solar Radiation Prediction Dashboard...\")",
        "detail": "run_frontend",
        "documentation": {}
    },
    {
        "label": "combined_df",
        "kind": 5,
        "importPath": "scores_graphs",
        "description": "scores_graphs",
        "peekOfCode": "combined_df = pd.DataFrame(\n    [\n        [\"XGBoost\", 0.9981, 0.9933, 22.6207, 22.6207, 9.6930, 9.6930],\n        [\"RF\", 0.9963, 0.9933, 22.7748, 22.7748, 9.8764, 9.8764],\n        [\"LSTM\", 0.9876, 0.9873, 31.2977, 31.2977, 20.1867, 20.1867],\n        [\"CNN-LSTM\", 0.9891, 0.9896, 28.2191, 28.2191, 17.0076, 17.0076],\n        [\"TFT\", 0.9959, 0.9472, 63.6943, 63.6943, 28.5793, 28.5793],\n        [\"TCN\", 0.9936, 0.9461, 64.3800, 64.3800, 28.8272, 28.8272],\n    ],\n    columns=[",
        "detail": "scores_graphs",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": "scores_graphs",
        "description": "scores_graphs",
        "peekOfCode": "metrics = {\n    \"RÂ²\": [\"Train RÂ²\", \"Test RÂ²\"],\n    \"RMSE\": [\"Train RMSE\", \"Test RMSE\"],\n    \"MAE\": [\"Train MAE\", \"Test MAE\"],\n}\n# ====== Global Plot Settings ======\nplt.rcParams.update({\"font.family\": \"serif\", \"font.size\": 12})\npalette = {\"Train\": \"#FF6347\", \"Test\": \"#32CD32\"}  # Red & Green\n# ====== Plotting ======\nfig, axes = plt.subplots(1, 3, figsize=(20, 6))",
        "detail": "scores_graphs",
        "documentation": {}
    },
    {
        "label": "palette",
        "kind": 5,
        "importPath": "scores_graphs",
        "description": "scores_graphs",
        "peekOfCode": "palette = {\"Train\": \"#FF6347\", \"Test\": \"#32CD32\"}  # Red & Green\n# ====== Plotting ======\nfig, axes = plt.subplots(1, 3, figsize=(20, 6))\naxes = axes.flatten()\nfor ax, (metric_name, cols) in zip(axes, metrics.items()):\n    df_melted = combined_df.melt(\n        id_vars=[\"Model\"], value_vars=cols, var_name=\"DataType\", value_name=metric_name\n    )\n    df_melted[\"Type\"] = df_melted[\"DataType\"].str.split(\" \").str[0]  # Train/Test\n    sns.barplot(",
        "detail": "scores_graphs",
        "documentation": {}
    },
    {
        "label": "axes",
        "kind": 5,
        "importPath": "scores_graphs",
        "description": "scores_graphs",
        "peekOfCode": "axes = axes.flatten()\nfor ax, (metric_name, cols) in zip(axes, metrics.items()):\n    df_melted = combined_df.melt(\n        id_vars=[\"Model\"], value_vars=cols, var_name=\"DataType\", value_name=metric_name\n    )\n    df_melted[\"Type\"] = df_melted[\"DataType\"].str.split(\" \").str[0]  # Train/Test\n    sns.barplot(\n        data=df_melted, x=\"Model\", y=metric_name, hue=\"Type\", palette=palette, ax=ax\n    )\n    ax.set_title(",
        "detail": "scores_graphs",
        "documentation": {}
    },
    {
        "label": "rmse",
        "kind": 2,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "def rmse(a, b):\n    return np.sqrt(mean_squared_error(a, b))\n# ---------- 1. Load data ----------\ndf = pd.read_csv(\"NASA meteriological and solar radiaton data/lahore_hourly_filled.csv\")\ndf.columns = df.columns.str.strip()\nif \"datetime\" not in df.columns:\n    raise ValueError(f\"'datetime' column not found\")\ndf[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"datetime\"])\ndf.set_index(\"datetime\", inplace=True)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "make_lag_features",
        "kind": 2,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "def make_lag_features(series_df, max_lag=24):\n    # Features for tree-based models (XGBoost, RandomForest)\n    # Order: time features, solar geometry, weather, derived features\n    cols_base = [\n        target_col,\n        # Time features\n        \"hour\",\n        \"month\",\n        \"day_of_year\",\n        \"hour_sin\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "create_sequences",
        "kind": 2,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "def create_sequences(df_in, seq_len=24):\n    \"\"\"Create sequences where model sees past radiation values in sequence.\"\"\"\n    Xs, ys, idxs = [], [], []\n    # Scale ALL features including SolarRadiation\n    all_arr = scaler_seq.transform(df_in)\n    for i in range(seq_len, len(df_in)):\n        # Sequence includes past SolarRadiation values (last column)\n        Xs.append(all_arr[i - seq_len : i])\n        # Target is scaled SolarRadiation at time i\n        ys.append(all_arr[i, -1])  # Last column is SolarRadiation",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport joblib\nimport tensorflow as tf",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "RANDOM_STATE",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "RANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntf.random.set_seed(RANDOM_STATE)\n# create save directory\nos.makedirs(\"saved_models\", exist_ok=True)\n# TF GPU memory growth (Colab safe)\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\nif gpus:\n    try:\n        for gpu in gpus:",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "gpus",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except Exception:\n        pass\ndef rmse(a, b):\n    return np.sqrt(mean_squared_error(a, b))\n# ---------- 1. Load data ----------",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df = pd.read_csv(\"NASA meteriological and solar radiaton data/lahore_hourly_filled.csv\")\ndf.columns = df.columns.str.strip()\nif \"datetime\" not in df.columns:\n    raise ValueError(f\"'datetime' column not found\")\ndf[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"datetime\"])\ndf.set_index(\"datetime\", inplace=True)\ndf = df.sort_index()\ndf = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df.columns",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df.columns = df.columns.str.strip()\nif \"datetime\" not in df.columns:\n    raise ValueError(f\"'datetime' column not found\")\ndf[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"datetime\"])\ndf.set_index(\"datetime\", inplace=True)\ndf = df.sort_index()\ndf = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())\nrequired = [",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df[\"datetime\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"datetime\"])\ndf.set_index(\"datetime\", inplace=True)\ndf = df.sort_index()\ndf = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())\nrequired = [\n    \"ClearSkyRadiation\",\n    \"SolarRadiation\",\n    \"DirectRadiation\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df = df.dropna(subset=[\"datetime\"])\ndf.set_index(\"datetime\", inplace=True)\ndf = df.sort_index()\ndf = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())\nrequired = [\n    \"ClearSkyRadiation\",\n    \"SolarRadiation\",\n    \"DirectRadiation\",\n    \"DiffuseRadiation\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df = df.sort_index()\ndf = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())\nrequired = [\n    \"ClearSkyRadiation\",\n    \"SolarRadiation\",\n    \"DirectRadiation\",\n    \"DiffuseRadiation\",\n    \"SolarZenith\",\n    \"Temperature\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())\nrequired = [\n    \"ClearSkyRadiation\",\n    \"SolarRadiation\",\n    \"DirectRadiation\",\n    \"DiffuseRadiation\",\n    \"SolarZenith\",\n    \"Temperature\",\n    \"HumiditySpecific\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "required",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "required = [\n    \"ClearSkyRadiation\",\n    \"SolarRadiation\",\n    \"DirectRadiation\",\n    \"DiffuseRadiation\",\n    \"SolarZenith\",\n    \"Temperature\",\n    \"HumiditySpecific\",\n    \"HumidityRelative\",\n    \"Pressure\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "missing",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "missing = [c for c in required if c not in df.columns]\nif missing:\n    raise ValueError(\"Missing columns: \", missing)\n# IMPORTANT: Use ALL hours (not just daylight) so model learns night = 0\ndf_day = df.copy()\ndf_day.dropna(subset=[\"SolarRadiation\"], inplace=True)\n# Add cyclical encoding for hour and month (helps model understand periodicity)\ndf_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"day_of_year\"] = df_day.index.dayofyear",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day = df.copy()\ndf_day.dropna(subset=[\"SolarRadiation\"], inplace=True)\n# Add cyclical encoding for hour and month (helps model understand periodicity)\ndf_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"day_of_year\"] = df_day.index.dayofyear\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"day_of_year\"] = df_day.index.dayofyear\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\n# Day of year cyclical encoding (captures seasonal variation, peak at summer solstice ~day 172)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"month\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"month\"] = df_day.index.month\ndf_day[\"day_of_year\"] = df_day.index.dayofyear\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\n# Day of year cyclical encoding (captures seasonal variation, peak at summer solstice ~day 172)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\n# Clearness Index: ratio of actual to clear sky radiation (0-1, indicates cloud cover)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"day_of_year\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"day_of_year\"] = df_day.index.dayofyear\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\n# Day of year cyclical encoding (captures seasonal variation, peak at summer solstice ~day 172)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\n# Clearness Index: ratio of actual to clear sky radiation (0-1, indicates cloud cover)\n# Avoid division by zero - set to 0 when ClearSkyRadiation is 0",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour_sin\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\n# Day of year cyclical encoding (captures seasonal variation, peak at summer solstice ~day 172)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\n# Clearness Index: ratio of actual to clear sky radiation (0-1, indicates cloud cover)\n# Avoid division by zero - set to 0 when ClearSkyRadiation is 0\ndf_day[\"ClearnessIndex\"] = np.where(",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour_cos\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\n# Day of year cyclical encoding (captures seasonal variation, peak at summer solstice ~day 172)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\n# Clearness Index: ratio of actual to clear sky radiation (0-1, indicates cloud cover)\n# Avoid division by zero - set to 0 when ClearSkyRadiation is 0\ndf_day[\"ClearnessIndex\"] = np.where(\n    df_day[\"ClearSkyRadiation\"] > 0,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"month_sin\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\n# Day of year cyclical encoding (captures seasonal variation, peak at summer solstice ~day 172)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\n# Clearness Index: ratio of actual to clear sky radiation (0-1, indicates cloud cover)\n# Avoid division by zero - set to 0 when ClearSkyRadiation is 0\ndf_day[\"ClearnessIndex\"] = np.where(\n    df_day[\"ClearSkyRadiation\"] > 0,\n    df_day[\"SolarRadiation\"] / df_day[\"ClearSkyRadiation\"],",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"month_cos\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\n# Day of year cyclical encoding (captures seasonal variation, peak at summer solstice ~day 172)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\n# Clearness Index: ratio of actual to clear sky radiation (0-1, indicates cloud cover)\n# Avoid division by zero - set to 0 when ClearSkyRadiation is 0\ndf_day[\"ClearnessIndex\"] = np.where(\n    df_day[\"ClearSkyRadiation\"] > 0,\n    df_day[\"SolarRadiation\"] / df_day[\"ClearSkyRadiation\"],\n    0,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"doy_sin\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\n# Clearness Index: ratio of actual to clear sky radiation (0-1, indicates cloud cover)\n# Avoid division by zero - set to 0 when ClearSkyRadiation is 0\ndf_day[\"ClearnessIndex\"] = np.where(\n    df_day[\"ClearSkyRadiation\"] > 0,\n    df_day[\"SolarRadiation\"] / df_day[\"ClearSkyRadiation\"],\n    0,\n)\ndf_day[\"ClearnessIndex\"] = df_day[\"ClearnessIndex\"].clip(0, 1)  # Clip to valid range",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"doy_cos\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\n# Clearness Index: ratio of actual to clear sky radiation (0-1, indicates cloud cover)\n# Avoid division by zero - set to 0 when ClearSkyRadiation is 0\ndf_day[\"ClearnessIndex\"] = np.where(\n    df_day[\"ClearSkyRadiation\"] > 0,\n    df_day[\"SolarRadiation\"] / df_day[\"ClearSkyRadiation\"],\n    0,\n)\ndf_day[\"ClearnessIndex\"] = df_day[\"ClearnessIndex\"].clip(0, 1)  # Clip to valid range\ntarget_col = \"SolarRadiation\"",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"ClearnessIndex\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"ClearnessIndex\"] = np.where(\n    df_day[\"ClearSkyRadiation\"] > 0,\n    df_day[\"SolarRadiation\"] / df_day[\"ClearSkyRadiation\"],\n    0,\n)\ndf_day[\"ClearnessIndex\"] = df_day[\"ClearnessIndex\"].clip(0, 1)  # Clip to valid range\ntarget_col = \"SolarRadiation\"\nprint(\"Total records (including night):\", len(df_day))\n# ---------- 2. Train/Test split ----------\nsplit_idx = int(len(df_day) * 0.8)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"ClearnessIndex\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"ClearnessIndex\"] = df_day[\"ClearnessIndex\"].clip(0, 1)  # Clip to valid range\ntarget_col = \"SolarRadiation\"\nprint(\"Total records (including night):\", len(df_day))\n# ---------- 2. Train/Test split ----------\nsplit_idx = int(len(df_day) * 0.8)\ntrain = df_day.iloc[:split_idx].copy()\ntest = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------\nMAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "target_col",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "target_col = \"SolarRadiation\"\nprint(\"Total records (including night):\", len(df_day))\n# ---------- 2. Train/Test split ----------\nsplit_idx = int(len(df_day) * 0.8)\ntrain = df_day.iloc[:split_idx].copy()\ntest = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------\nMAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):\n    # Features for tree-based models (XGBoost, RandomForest)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "split_idx",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "split_idx = int(len(df_day) * 0.8)\ntrain = df_day.iloc[:split_idx].copy()\ntest = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------\nMAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):\n    # Features for tree-based models (XGBoost, RandomForest)\n    # Order: time features, solar geometry, weather, derived features\n    cols_base = [\n        target_col,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "train = df_day.iloc[:split_idx].copy()\ntest = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------\nMAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):\n    # Features for tree-based models (XGBoost, RandomForest)\n    # Order: time features, solar geometry, weather, derived features\n    cols_base = [\n        target_col,\n        # Time features",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "test = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------\nMAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):\n    # Features for tree-based models (XGBoost, RandomForest)\n    # Order: time features, solar geometry, weather, derived features\n    cols_base = [\n        target_col,\n        # Time features\n        \"hour\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "MAX_LAG",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "MAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):\n    # Features for tree-based models (XGBoost, RandomForest)\n    # Order: time features, solar geometry, weather, derived features\n    cols_base = [\n        target_col,\n        # Time features\n        \"hour\",\n        \"month\",\n        \"day_of_year\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_lag",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_lag = make_lag_features(df_day, MAX_LAG)\ntrain_lag = df_lag.loc[df_lag.index.intersection(train.index)]\ntest_lag = df_lag.loc[df_lag.index.intersection(test.index)]\n# fallback if no test rows\nif len(test_lag) == 0:\n    train_lag = df_lag.iloc[:-MAX_LAG].copy()\n    test_lag = df_lag.iloc[-MAX_LAG:].copy()\nX_train_tree = train_lag.drop(columns=[target_col])\ny_train_tree = train_lag[target_col]\nX_test_tree = test_lag.drop(columns=[target_col])",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "train_lag",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "train_lag = df_lag.loc[df_lag.index.intersection(train.index)]\ntest_lag = df_lag.loc[df_lag.index.intersection(test.index)]\n# fallback if no test rows\nif len(test_lag) == 0:\n    train_lag = df_lag.iloc[:-MAX_LAG].copy()\n    test_lag = df_lag.iloc[-MAX_LAG:].copy()\nX_train_tree = train_lag.drop(columns=[target_col])\ny_train_tree = train_lag[target_col]\nX_test_tree = test_lag.drop(columns=[target_col])\ny_test = test_lag[target_col]",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "test_lag",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "test_lag = df_lag.loc[df_lag.index.intersection(test.index)]\n# fallback if no test rows\nif len(test_lag) == 0:\n    train_lag = df_lag.iloc[:-MAX_LAG].copy()\n    test_lag = df_lag.iloc[-MAX_LAG:].copy()\nX_train_tree = train_lag.drop(columns=[target_col])\ny_train_tree = train_lag[target_col]\nX_test_tree = test_lag.drop(columns=[target_col])\ny_test = test_lag[target_col]\n# ---------- 4. Scaling for deep learning ----------",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "X_train_tree",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "X_train_tree = train_lag.drop(columns=[target_col])\ny_train_tree = train_lag[target_col]\nX_test_tree = test_lag.drop(columns=[target_col])\ny_test = test_lag[target_col]\n# ---------- 4. Scaling for deep learning ----------\nSEQ_LEN = 24\n# Features for sequence models (LSTM, CNN-LSTM)\n# KEY: Include SolarRadiation so model can learn from past radiation values (like tree models)\nfeatures_seq = [\n    # Time features",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_train_tree",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_train_tree = train_lag[target_col]\nX_test_tree = test_lag.drop(columns=[target_col])\ny_test = test_lag[target_col]\n# ---------- 4. Scaling for deep learning ----------\nSEQ_LEN = 24\n# Features for sequence models (LSTM, CNN-LSTM)\n# KEY: Include SolarRadiation so model can learn from past radiation values (like tree models)\nfeatures_seq = [\n    # Time features\n    \"hour_sin\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "X_test_tree",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "X_test_tree = test_lag.drop(columns=[target_col])\ny_test = test_lag[target_col]\n# ---------- 4. Scaling for deep learning ----------\nSEQ_LEN = 24\n# Features for sequence models (LSTM, CNN-LSTM)\n# KEY: Include SolarRadiation so model can learn from past radiation values (like tree models)\nfeatures_seq = [\n    # Time features\n    \"hour_sin\",\n    \"hour_cos\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_test",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_test = test_lag[target_col]\n# ---------- 4. Scaling for deep learning ----------\nSEQ_LEN = 24\n# Features for sequence models (LSTM, CNN-LSTM)\n# KEY: Include SolarRadiation so model can learn from past radiation values (like tree models)\nfeatures_seq = [\n    # Time features\n    \"hour_sin\",\n    \"hour_cos\",\n    \"month_sin\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "SEQ_LEN",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "SEQ_LEN = 24\n# Features for sequence models (LSTM, CNN-LSTM)\n# KEY: Include SolarRadiation so model can learn from past radiation values (like tree models)\nfeatures_seq = [\n    # Time features\n    \"hour_sin\",\n    \"hour_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"doy_sin\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "features_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "features_seq = [\n    # Time features\n    \"hour_sin\",\n    \"hour_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"doy_sin\",\n    \"doy_cos\",\n    # Solar geometry\n    \"SolarZenith\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_seq = df_day[features_seq].dropna()\ntrain_seq_df = df_seq.loc[: train.index.max()]\n# Scale ALL features including SolarRadiation for sequences\nscaler_seq = StandardScaler()\nscaler_seq.fit(train_seq_df)\n# Separate scaler for target (for inverse transform)\nscaler_y = StandardScaler()\nscaler_y.fit(train_seq_df[[target_col]])\n# Also keep scaler_X for compatibility with TFT/TCN\nscaler_X = StandardScaler()",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "train_seq_df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "train_seq_df = df_seq.loc[: train.index.max()]\n# Scale ALL features including SolarRadiation for sequences\nscaler_seq = StandardScaler()\nscaler_seq.fit(train_seq_df)\n# Separate scaler for target (for inverse transform)\nscaler_y = StandardScaler()\nscaler_y.fit(train_seq_df[[target_col]])\n# Also keep scaler_X for compatibility with TFT/TCN\nscaler_X = StandardScaler()\nscaler_X.fit(train_seq_df.drop(columns=[target_col]))",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "scaler_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "scaler_seq = StandardScaler()\nscaler_seq.fit(train_seq_df)\n# Separate scaler for target (for inverse transform)\nscaler_y = StandardScaler()\nscaler_y.fit(train_seq_df[[target_col]])\n# Also keep scaler_X for compatibility with TFT/TCN\nscaler_X = StandardScaler()\nscaler_X.fit(train_seq_df.drop(columns=[target_col]))\n# Save scalers\njoblib.dump(scaler_X, \"saved_models/scaler_X.pkl\")",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "scaler_y",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "scaler_y = StandardScaler()\nscaler_y.fit(train_seq_df[[target_col]])\n# Also keep scaler_X for compatibility with TFT/TCN\nscaler_X = StandardScaler()\nscaler_X.fit(train_seq_df.drop(columns=[target_col]))\n# Save scalers\njoblib.dump(scaler_X, \"saved_models/scaler_X.pkl\")\njoblib.dump(scaler_y, \"saved_models/scaler_y.pkl\")\njoblib.dump(scaler_seq, \"saved_models/scaler_seq.pkl\")\ndef create_sequences(df_in, seq_len=24):",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "scaler_X",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "scaler_X = StandardScaler()\nscaler_X.fit(train_seq_df.drop(columns=[target_col]))\n# Save scalers\njoblib.dump(scaler_X, \"saved_models/scaler_X.pkl\")\njoblib.dump(scaler_y, \"saved_models/scaler_y.pkl\")\njoblib.dump(scaler_seq, \"saved_models/scaler_seq.pkl\")\ndef create_sequences(df_in, seq_len=24):\n    \"\"\"Create sequences where model sees past radiation values in sequence.\"\"\"\n    Xs, ys, idxs = [], [], []\n    # Scale ALL features including SolarRadiation",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "train_idx_set",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "train_idx_set = set(train.index)\ntest_idx_set = set(test.index)\ntrain_mask = np.array([idx in train_idx_set for idx in idxs_all])\ntest_mask = np.array([idx in test_idx_set for idx in idxs_all])\nX_train_seq = X_seq_all[train_mask]\ny_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "test_idx_set",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "test_idx_set = set(test.index)\ntrain_mask = np.array([idx in train_idx_set for idx in idxs_all])\ntest_mask = np.array([idx in test_idx_set for idx in idxs_all])\nX_train_seq = X_seq_all[train_mask]\ny_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "train_mask",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "train_mask = np.array([idx in train_idx_set for idx in idxs_all])\ntest_mask = np.array([idx in test_idx_set for idx in idxs_all])\nX_train_seq = X_seq_all[train_mask]\ny_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "test_mask",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "test_mask = np.array([idx in test_idx_set for idx in idxs_all])\nX_train_seq = X_seq_all[train_mask]\ny_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "X_train_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "X_train_seq = X_seq_all[train_mask]\ny_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_train_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost\n# ======================================",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "X_test_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "X_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost\n# ======================================\nprint(\"\\n=== Training XGBoost ===\")",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_test_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost\n# ======================================\nprint(\"\\n=== Training XGBoost ===\")\nxgb_model = xgb.XGBRegressor(",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_train_unscaled",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost\n# ======================================\nprint(\"\\n=== Training XGBoost ===\")\nxgb_model = xgb.XGBRegressor(\n    n_estimators=800,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_test_unscaled",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost\n# ======================================\nprint(\"\\n=== Training XGBoost ===\")\nxgb_model = xgb.XGBRegressor(\n    n_estimators=800,\n    max_depth=5,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "results = []\n# ======================================\n# 6. XGBoost\n# ======================================\nprint(\"\\n=== Training XGBoost ===\")\nxgb_model = xgb.XGBRegressor(\n    n_estimators=800,\n    max_depth=5,\n    learning_rate=0.05,\n    subsample=0.8,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "xgb_model",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "xgb_model = xgb.XGBRegressor(\n    n_estimators=800,\n    max_depth=5,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=RANDOM_STATE,\n    n_jobs=-1,\n)\nxgb_model.fit(X_train_tree, y_train_tree)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "xgb_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "xgb_train_pred = xgb_model.predict(X_train_tree)\nxgb_test_pred = xgb_model.predict(X_test_tree)\nresults.append(\n    [\n        \"XGBoost\",\n        r2_score(y_train_tree, xgb_train_pred),\n        r2_score(y_test, xgb_test_pred),\n        mean_absolute_error(y_test, xgb_test_pred),\n        rmse(y_test, xgb_test_pred),\n    ]",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "xgb_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "xgb_test_pred = xgb_model.predict(X_test_tree)\nresults.append(\n    [\n        \"XGBoost\",\n        r2_score(y_train_tree, xgb_train_pred),\n        r2_score(y_test, xgb_test_pred),\n        mean_absolute_error(y_test, xgb_test_pred),\n        rmse(y_test, xgb_test_pred),\n    ]\n)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "rf",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "rf = RandomForestRegressor(\n    n_estimators=300, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1\n)\nrf.fit(X_train_tree, y_train_tree)\n# Save model\njoblib.dump(rf, \"saved_models/random_forest_model.pkl\")\nrf_train_pred = rf.predict(X_train_tree)\nrf_test_pred = rf.predict(X_test_tree)\nresults.append(\n    [",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "rf_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "rf_train_pred = rf.predict(X_train_tree)\nrf_test_pred = rf.predict(X_test_tree)\nresults.append(\n    [\n        \"Random Forest\",\n        r2_score(y_train_tree, rf_train_pred),\n        r2_score(y_test, rf_test_pred),\n        mean_absolute_error(y_test, rf_test_pred),\n        rmse(y_test, rf_test_pred),\n    ]",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "rf_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "rf_test_pred = rf.predict(X_test_tree)\nresults.append(\n    [\n        \"Random Forest\",\n        r2_score(y_train_tree, rf_train_pred),\n        r2_score(y_test, rf_test_pred),\n        mean_absolute_error(y_test, rf_test_pred),\n        rmse(y_test, rf_test_pred),\n    ]\n)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lstm_model",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lstm_model = models.Sequential(\n    [\n        layers.Input(shape=(SEQ_LEN, X_train_seq.shape[2])),\n        # First LSTM layer - captures temporal patterns\n        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n        layers.Dropout(0.2),\n        # Second LSTM layer - extracts higher-level features\n        layers.Bidirectional(layers.LSTM(32)),\n        layers.Dropout(0.2),\n        # Dense layers for regression",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lr_schedule",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lr_schedule = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=0\n)\nes = callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_loss\")\nlstm_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\nlstm_model.fit(",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "es",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "es = callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_loss\")\nlstm_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\nlstm_model.fit(\n    X_train_seq,\n    y_train_seq,\n    validation_split=0.1,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lstm_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lstm_train_pred = scaler_y.inverse_transform(lstm_model.predict(X_train_seq))\nlstm_test_pred = scaler_y.inverse_transform(lstm_model.predict(X_test_seq))\nlstm_train_pred = lstm_train_pred.flatten()\nlstm_test_pred = lstm_test_pred.flatten()\nresults.append(\n    [\n        \"LSTM\",\n        r2_score(y_train_unscaled, lstm_train_pred),\n        r2_score(y_test_unscaled, lstm_test_pred),\n        mean_absolute_error(y_test_unscaled, lstm_test_pred),",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lstm_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lstm_test_pred = scaler_y.inverse_transform(lstm_model.predict(X_test_seq))\nlstm_train_pred = lstm_train_pred.flatten()\nlstm_test_pred = lstm_test_pred.flatten()\nresults.append(\n    [\n        \"LSTM\",\n        r2_score(y_train_unscaled, lstm_train_pred),\n        r2_score(y_test_unscaled, lstm_test_pred),\n        mean_absolute_error(y_test_unscaled, lstm_test_pred),\n        rmse(y_test_unscaled, lstm_test_pred),",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lstm_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lstm_train_pred = lstm_train_pred.flatten()\nlstm_test_pred = lstm_test_pred.flatten()\nresults.append(\n    [\n        \"LSTM\",\n        r2_score(y_train_unscaled, lstm_train_pred),\n        r2_score(y_test_unscaled, lstm_test_pred),\n        mean_absolute_error(y_test_unscaled, lstm_test_pred),\n        rmse(y_test_unscaled, lstm_test_pred),\n    ]",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lstm_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lstm_test_pred = lstm_test_pred.flatten()\nresults.append(\n    [\n        \"LSTM\",\n        r2_score(y_train_unscaled, lstm_train_pred),\n        r2_score(y_test_unscaled, lstm_test_pred),\n        mean_absolute_error(y_test_unscaled, lstm_test_pred),\n        rmse(y_test_unscaled, lstm_test_pred),\n    ]\n)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "inp = layers.Input(shape=(SEQ_LEN, X_train_seq.shape[2]))\n# Conv layers to extract local patterns\nx = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\")(inp)\nx = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\")(x)\nx = layers.MaxPool1D(2)(x)\nx = layers.Dropout(0.2)(x)\n# LSTM layers for temporal dependencies\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Bidirectional(layers.LSTM(32))(x)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\")(inp)\nx = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\")(x)\nx = layers.MaxPool1D(2)(x)\nx = layers.Dropout(0.2)(x)\n# LSTM layers for temporal dependencies\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Bidirectional(layers.LSTM(32))(x)\nx = layers.Dropout(0.2)(x)\n# Dense output layers",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\")(x)\nx = layers.MaxPool1D(2)(x)\nx = layers.Dropout(0.2)(x)\n# LSTM layers for temporal dependencies\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Bidirectional(layers.LSTM(32))(x)\nx = layers.Dropout(0.2)(x)\n# Dense output layers\nx = layers.Dense(64, activation=\"relu\")(x)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.MaxPool1D(2)(x)\nx = layers.Dropout(0.2)(x)\n# LSTM layers for temporal dependencies\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Bidirectional(layers.LSTM(32))(x)\nx = layers.Dropout(0.2)(x)\n# Dense output layers\nx = layers.Dense(64, activation=\"relu\")(x)\nx = layers.Dense(32, activation=\"relu\")(x)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Dropout(0.2)(x)\n# LSTM layers for temporal dependencies\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Bidirectional(layers.LSTM(32))(x)\nx = layers.Dropout(0.2)(x)\n# Dense output layers\nx = layers.Dense(64, activation=\"relu\")(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nout = layers.Dense(1)(x)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Bidirectional(layers.LSTM(32))(x)\nx = layers.Dropout(0.2)(x)\n# Dense output layers\nx = layers.Dense(64, activation=\"relu\")(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\n# Learning rate scheduler",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Dropout(0.2)(x)\nx = layers.Bidirectional(layers.LSTM(32))(x)\nx = layers.Dropout(0.2)(x)\n# Dense output layers\nx = layers.Dense(64, activation=\"relu\")(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\n# Learning rate scheduler\nlr_schedule_cnn = callbacks.ReduceLROnPlateau(",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Bidirectional(layers.LSTM(32))(x)\nx = layers.Dropout(0.2)(x)\n# Dense output layers\nx = layers.Dense(64, activation=\"relu\")(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\n# Learning rate scheduler\nlr_schedule_cnn = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=0",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Dropout(0.2)(x)\n# Dense output layers\nx = layers.Dense(64, activation=\"relu\")(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\n# Learning rate scheduler\nlr_schedule_cnn = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=0\n)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Dense(64, activation=\"relu\")(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\n# Learning rate scheduler\nlr_schedule_cnn = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=0\n)\nes_cnn = callbacks.EarlyStopping(\n    patience=15, restore_best_weights=True, monitor=\"val_loss\"",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Dense(32, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\n# Learning rate scheduler\nlr_schedule_cnn = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=0\n)\nes_cnn = callbacks.EarlyStopping(\n    patience=15, restore_best_weights=True, monitor=\"val_loss\"\n)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "out",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "out = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\n# Learning rate scheduler\nlr_schedule_cnn = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=0\n)\nes_cnn = callbacks.EarlyStopping(\n    patience=15, restore_best_weights=True, monitor=\"val_loss\"\n)\ncnn_lstm_model.compile(",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "cnn_lstm_model",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "cnn_lstm_model = models.Model(inp, out)\n# Learning rate scheduler\nlr_schedule_cnn = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=0\n)\nes_cnn = callbacks.EarlyStopping(\n    patience=15, restore_best_weights=True, monitor=\"val_loss\"\n)\ncnn_lstm_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lr_schedule_cnn",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lr_schedule_cnn = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=0\n)\nes_cnn = callbacks.EarlyStopping(\n    patience=15, restore_best_weights=True, monitor=\"val_loss\"\n)\ncnn_lstm_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss=\"mse\",\n    metrics=[\"mae\"],",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "es_cnn",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "es_cnn = callbacks.EarlyStopping(\n    patience=15, restore_best_weights=True, monitor=\"val_loss\"\n)\ncnn_lstm_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss=\"mse\",\n    metrics=[\"mae\"],\n)\ncnn_lstm_model.fit(\n    X_train_seq,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "cnn_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "cnn_train_pred = scaler_y.inverse_transform(cnn_lstm_model.predict(X_train_seq))\ncnn_test_pred = scaler_y.inverse_transform(cnn_lstm_model.predict(X_test_seq))\ncnn_train_pred = cnn_train_pred.flatten()\ncnn_test_pred = cnn_test_pred.flatten()\nresults.append(\n    [\n        \"CNN-LSTM\",\n        r2_score(y_train_unscaled, cnn_train_pred),\n        r2_score(y_test_unscaled, cnn_test_pred),\n        mean_absolute_error(y_test_unscaled, cnn_test_pred),",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "cnn_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "cnn_test_pred = scaler_y.inverse_transform(cnn_lstm_model.predict(X_test_seq))\ncnn_train_pred = cnn_train_pred.flatten()\ncnn_test_pred = cnn_test_pred.flatten()\nresults.append(\n    [\n        \"CNN-LSTM\",\n        r2_score(y_train_unscaled, cnn_train_pred),\n        r2_score(y_test_unscaled, cnn_test_pred),\n        mean_absolute_error(y_test_unscaled, cnn_test_pred),\n        rmse(y_test_unscaled, cnn_test_pred),",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "cnn_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "cnn_train_pred = cnn_train_pred.flatten()\ncnn_test_pred = cnn_test_pred.flatten()\nresults.append(\n    [\n        \"CNN-LSTM\",\n        r2_score(y_train_unscaled, cnn_train_pred),\n        r2_score(y_test_unscaled, cnn_test_pred),\n        mean_absolute_error(y_test_unscaled, cnn_test_pred),\n        rmse(y_test_unscaled, cnn_test_pred),\n    ]",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "cnn_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "cnn_test_pred = cnn_test_pred.flatten()\nresults.append(\n    [\n        \"CNN-LSTM\",\n        r2_score(y_train_unscaled, cnn_train_pred),\n        r2_score(y_test_unscaled, cnn_test_pred),\n        mean_absolute_error(y_test_unscaled, cnn_test_pred),\n        rmse(y_test_unscaled, cnn_test_pred),\n    ]\n)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "results_df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "results_df = pd.DataFrame(\n    results, columns=[\"Model\", \"R2_Train\", \"R2_Test\", \"MAE_Test\", \"RMSE_Test\"]\n)\nprint(results_df)\nresults_df.to_csv(\"saved_models/model_results.csv\", index=False)\n# ======================================\n# 11. Ensemble Model (Weighted Average)\n# ======================================\nprint(\"\\n=== Creating Ensemble Model ===\")\n# Calculate weights based on test R2 scores (better models get higher weight)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "r2_scores",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "r2_scores = {\n    \"XGBoost\": r2_score(y_test, xgb_test_pred),\n    \"RandomForest\": r2_score(y_test, rf_test_pred),\n    \"LSTM\": r2_score(y_test_unscaled, lstm_test_pred),\n    \"CNN-LSTM\": r2_score(y_test_unscaled, cnn_test_pred),\n}\n# Normalize weights (ensure they sum to 1)\ntotal_r2 = sum(max(0, r2) for r2 in r2_scores.values())\nensemble_weights = {k: max(0, v) / total_r2 for k, v in r2_scores.items()}\nprint(\"Ensemble weights:\", {k: f\"{v:.3f}\" for k, v in ensemble_weights.items()})",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "total_r2",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "total_r2 = sum(max(0, r2) for r2 in r2_scores.values())\nensemble_weights = {k: max(0, v) / total_r2 for k, v in r2_scores.items()}\nprint(\"Ensemble weights:\", {k: f\"{v:.3f}\" for k, v in ensemble_weights.items()})\n# Save ensemble weights for prediction\njoblib.dump(ensemble_weights, \"saved_models/ensemble_weights.pkl\")\n# Note: Ensemble prediction requires aligning predictions from all models\n# For now, we save weights; ensemble will be computed during prediction in trend.py\nprint(\"\\nðŸŽ‰ All models saved successfully in: saved_models/\")",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "ensemble_weights",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "ensemble_weights = {k: max(0, v) / total_r2 for k, v in r2_scores.items()}\nprint(\"Ensemble weights:\", {k: f\"{v:.3f}\" for k, v in ensemble_weights.items()})\n# Save ensemble weights for prediction\njoblib.dump(ensemble_weights, \"saved_models/ensemble_weights.pkl\")\n# Note: Ensemble prediction requires aligning predictions from all models\n# For now, we save weights; ensemble will be computed during prediction in trend.py\nprint(\"\\nðŸŽ‰ All models saved successfully in: saved_models/\")",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "fetch_openmeteo_solar_forecast",
        "kind": 2,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "def fetch_openmeteo_solar_forecast(year, month, day):\n    \"\"\"\n    Fetch solar radiation forecast from Open-Meteo API.\n    âœ… FREE, No API key required\n    ðŸ“Š Returns hourly GHI (Global Horizontal Irradiance) in W/mÂ².\n    â° Forecast only (today + 16 days)\n    \"\"\"\n    date_str = f\"{year}-{month:02d}-{day:02d}\"\n    url = \"https://api.open-meteo.com/v1/forecast\"\n    params = {",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "fetch_nasa_power_solar",
        "kind": 2,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "def fetch_nasa_power_solar(year, month, day):\n    \"\"\"\n    Fetch solar radiation data from NASA POWER API.\n    âœ… FREE, No API key required\n    ðŸ“Š Returns hourly ALLSKY_SFC_SW_DWN (Surface Shortwave Downward Irradiance) in W/mÂ².\n    â° Historical data (up to ~1 week ago) - good for validation\n    \"\"\"\n    date_str = f\"{year}{month:02d}{day:02d}\"\n    url = \"https://power.larc.nasa.gov/api/temporal/hourly/point\"\n    params = {",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "fetch_api_prediction",
        "kind": 2,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "def fetch_api_prediction(year, month, day, api_name):\n    \"\"\"\n    Unified function to fetch solar radiation from selected API.\n    \"\"\"\n    api_name = api_name.lower()\n    if api_name == \"open-meteo\":\n        return fetch_openmeteo_solar_forecast(year, month, day), \"Open-Meteo\"\n    elif api_name == \"nasa-power\":\n        return fetch_nasa_power_solar(year, month, day), \"NASA POWER\"\n    else:",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "list_available_apis",
        "kind": 2,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "def list_available_apis():\n    \"\"\"Print available APIs and their status.\"\"\"\n    print(\"\\nðŸ“¡ Available APIs:\")\n    print(\"  1. open-meteo - âœ… FREE, No key (Forecast: today + 16 days)\")\n    print(\"  2. nasa-power - âœ… FREE, No key (Historical, ~1 week delay)\")\n    print(f\"\\n  Currently selected: {SELECTED_API}\\n\")\n# ---------- 1. Load historical data ----------\ndf = pd.read_csv(\n    \"NASA meteriological and solar radiaton data/lahore_hourly_filled.csv\",\n    parse_dates=[\"datetime\"],",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport requests\nfrom tensorflow.keras.models import load_model\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n# ============== API Configuration ==============\n# Lahore, Pakistan coordinates",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "LATITUDE",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "LATITUDE = 31.56\nLONGITUDE = 74.35\nTIMEZONE = \"Asia/Karachi\"\n# ============== API Selection ==============\n# Available APIs: \"open-meteo\", \"nasa-power\"\nSELECTED_API = \"open-meteo\"  # <-- Testing Open-Meteo configuration\n# ============== API Functions ==============\ndef fetch_openmeteo_solar_forecast(year, month, day):\n    \"\"\"\n    Fetch solar radiation forecast from Open-Meteo API.",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "LONGITUDE",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "LONGITUDE = 74.35\nTIMEZONE = \"Asia/Karachi\"\n# ============== API Selection ==============\n# Available APIs: \"open-meteo\", \"nasa-power\"\nSELECTED_API = \"open-meteo\"  # <-- Testing Open-Meteo configuration\n# ============== API Functions ==============\ndef fetch_openmeteo_solar_forecast(year, month, day):\n    \"\"\"\n    Fetch solar radiation forecast from Open-Meteo API.\n    âœ… FREE, No API key required",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "TIMEZONE",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "TIMEZONE = \"Asia/Karachi\"\n# ============== API Selection ==============\n# Available APIs: \"open-meteo\", \"nasa-power\"\nSELECTED_API = \"open-meteo\"  # <-- Testing Open-Meteo configuration\n# ============== API Functions ==============\ndef fetch_openmeteo_solar_forecast(year, month, day):\n    \"\"\"\n    Fetch solar radiation forecast from Open-Meteo API.\n    âœ… FREE, No API key required\n    ðŸ“Š Returns hourly GHI (Global Horizontal Irradiance) in W/mÂ².",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "SELECTED_API",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "SELECTED_API = \"open-meteo\"  # <-- Testing Open-Meteo configuration\n# ============== API Functions ==============\ndef fetch_openmeteo_solar_forecast(year, month, day):\n    \"\"\"\n    Fetch solar radiation forecast from Open-Meteo API.\n    âœ… FREE, No API key required\n    ðŸ“Š Returns hourly GHI (Global Horizontal Irradiance) in W/mÂ².\n    â° Forecast only (today + 16 days)\n    \"\"\"\n    date_str = f\"{year}-{month:02d}-{day:02d}\"",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df = pd.read_csv(\n    \"NASA meteriological and solar radiaton data/lahore_hourly_filled.csv\",\n    parse_dates=[\"datetime\"],\n    dayfirst=True,\n    index_col=\"datetime\",\n)\ndf = df.sort_index().apply(pd.to_numeric, errors=\"coerce\")\ndf.index = pd.to_datetime(df.index, dayfirst=True)\n# Use ALL hours (not just daylight) - model should know night = 0\ndf_day = df.copy()",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df = df.sort_index().apply(pd.to_numeric, errors=\"coerce\")\ndf.index = pd.to_datetime(df.index, dayfirst=True)\n# Use ALL hours (not just daylight) - model should know night = 0\ndf_day = df.copy()\ndf_day.dropna(subset=[\"SolarRadiation\"], inplace=True)\n# Add cyclical encoding for hour and month\ndf_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"day_of_year\"] = df_day.index.dayofyear\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df.index",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df.index = pd.to_datetime(df.index, dayfirst=True)\n# Use ALL hours (not just daylight) - model should know night = 0\ndf_day = df.copy()\ndf_day.dropna(subset=[\"SolarRadiation\"], inplace=True)\n# Add cyclical encoding for hour and month\ndf_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"day_of_year\"] = df_day.index.dayofyear\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day = df.copy()\ndf_day.dropna(subset=[\"SolarRadiation\"], inplace=True)\n# Add cyclical encoding for hour and month\ndf_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"day_of_year\"] = df_day.index.dayofyear\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"day_of_year\"] = df_day.index.dayofyear\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\ntarget_col = \"SolarRadiation\"",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"month\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"month\"] = df_day.index.month\ndf_day[\"day_of_year\"] = df_day.index.dayofyear\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"day_of_year\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"day_of_year\"] = df_day.index.dayofyear\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\n# Using saved_models_lstm folder (best LSTM/CNN-LSTM scores)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour_sin\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\n# Using saved_models_lstm folder (best LSTM/CNN-LSTM scores)\nMODEL_FOLDER = \"saved_models_lstm\"",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour_cos\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\n# Using saved_models_lstm folder (best LSTM/CNN-LSTM scores)\nMODEL_FOLDER = \"saved_models_lstm\"\nxgb_model = joblib.load(f\"{MODEL_FOLDER}/xgboost_model.pkl\")",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"month_sin\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\n# Using saved_models_lstm folder (best LSTM/CNN-LSTM scores)\nMODEL_FOLDER = \"saved_models_lstm\"\nxgb_model = joblib.load(f\"{MODEL_FOLDER}/xgboost_model.pkl\")\nrf_model = joblib.load(f\"{MODEL_FOLDER}/random_forest_model.pkl\")",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"month_cos\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\n# Using saved_models_lstm folder (best LSTM/CNN-LSTM scores)\nMODEL_FOLDER = \"saved_models_lstm\"\nxgb_model = joblib.load(f\"{MODEL_FOLDER}/xgboost_model.pkl\")\nrf_model = joblib.load(f\"{MODEL_FOLDER}/random_forest_model.pkl\")\n# Load Keras 3.x format models (converted from h5)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"doy_sin\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"doy_sin\"] = np.sin(2 * np.pi * df_day[\"day_of_year\"] / 365)\ndf_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\n# Using saved_models_lstm folder (best LSTM/CNN-LSTM scores)\nMODEL_FOLDER = \"saved_models_lstm\"\nxgb_model = joblib.load(f\"{MODEL_FOLDER}/xgboost_model.pkl\")\nrf_model = joblib.load(f\"{MODEL_FOLDER}/random_forest_model.pkl\")\n# Load Keras 3.x format models (converted from h5)\nimport os",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"doy_cos\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"doy_cos\"] = np.cos(2 * np.pi * df_day[\"day_of_year\"] / 365)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\n# Using saved_models_lstm folder (best LSTM/CNN-LSTM scores)\nMODEL_FOLDER = \"saved_models_lstm\"\nxgb_model = joblib.load(f\"{MODEL_FOLDER}/xgboost_model.pkl\")\nrf_model = joblib.load(f\"{MODEL_FOLDER}/random_forest_model.pkl\")\n# Load Keras 3.x format models (converted from h5)\nimport os\nif os.path.exists(f\"{MODEL_FOLDER}/lstm_model_v3.keras\"):",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "target_col",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "target_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\n# Using saved_models_lstm folder (best LSTM/CNN-LSTM scores)\nMODEL_FOLDER = \"saved_models_lstm\"\nxgb_model = joblib.load(f\"{MODEL_FOLDER}/xgboost_model.pkl\")\nrf_model = joblib.load(f\"{MODEL_FOLDER}/random_forest_model.pkl\")\n# Load Keras 3.x format models (converted from h5)\nimport os\nif os.path.exists(f\"{MODEL_FOLDER}/lstm_model_v3.keras\"):\n    lstm_model = load_model(f\"{MODEL_FOLDER}/lstm_model_v3.keras\")",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "MODEL_FOLDER",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "MODEL_FOLDER = \"saved_models_lstm\"\nxgb_model = joblib.load(f\"{MODEL_FOLDER}/xgboost_model.pkl\")\nrf_model = joblib.load(f\"{MODEL_FOLDER}/random_forest_model.pkl\")\n# Load Keras 3.x format models (converted from h5)\nimport os\nif os.path.exists(f\"{MODEL_FOLDER}/lstm_model_v3.keras\"):\n    lstm_model = load_model(f\"{MODEL_FOLDER}/lstm_model_v3.keras\")\nelse:\n    lstm_model = load_model(f\"{MODEL_FOLDER}/lstm_model.h5\")\nif os.path.exists(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\"):",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "xgb_model",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "xgb_model = joblib.load(f\"{MODEL_FOLDER}/xgboost_model.pkl\")\nrf_model = joblib.load(f\"{MODEL_FOLDER}/random_forest_model.pkl\")\n# Load Keras 3.x format models (converted from h5)\nimport os\nif os.path.exists(f\"{MODEL_FOLDER}/lstm_model_v3.keras\"):\n    lstm_model = load_model(f\"{MODEL_FOLDER}/lstm_model_v3.keras\")\nelse:\n    lstm_model = load_model(f\"{MODEL_FOLDER}/lstm_model.h5\")\nif os.path.exists(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\"):\n    cnn_lstm_model = load_model(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\")",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "rf_model",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "rf_model = joblib.load(f\"{MODEL_FOLDER}/random_forest_model.pkl\")\n# Load Keras 3.x format models (converted from h5)\nimport os\nif os.path.exists(f\"{MODEL_FOLDER}/lstm_model_v3.keras\"):\n    lstm_model = load_model(f\"{MODEL_FOLDER}/lstm_model_v3.keras\")\nelse:\n    lstm_model = load_model(f\"{MODEL_FOLDER}/lstm_model.h5\")\nif os.path.exists(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\"):\n    cnn_lstm_model = load_model(f\"{MODEL_FOLDER}/cnn_lstm_model_v3.keras\")\nelse:",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "scaler_X",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "scaler_X = joblib.load(f\"{MODEL_FOLDER}/scaler_X.pkl\")\nscaler_y = joblib.load(f\"{MODEL_FOLDER}/scaler_y.pkl\")\n# Load ensemble weights (original from training)\ntry:\n    ensemble_weights = joblib.load(f\"{MODEL_FOLDER}/ensemble_weights.pkl\")\n    print(f\"Ensemble weights loaded: {ensemble_weights}\")\nexcept:\n    ensemble_weights = {\n        \"XGBoost\": 0.25,\n        \"RandomForest\": 0.25,",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "scaler_y",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "scaler_y = joblib.load(f\"{MODEL_FOLDER}/scaler_y.pkl\")\n# Load ensemble weights (original from training)\ntry:\n    ensemble_weights = joblib.load(f\"{MODEL_FOLDER}/ensemble_weights.pkl\")\n    print(f\"Ensemble weights loaded: {ensemble_weights}\")\nexcept:\n    ensemble_weights = {\n        \"XGBoost\": 0.25,\n        \"RandomForest\": 0.25,\n        \"LSTM\": 0.25,",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "NASA_ENSEMBLE_WEIGHTS",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "NASA_ENSEMBLE_WEIGHTS = {\n    \"XGBoost\": 0.45,  # Best performer with NASA (RÂ² = 0.958)\n    \"RandomForest\": 0.40,  # Second best (RÂ² = 0.940)\n    \"LSTM\": 0.08,  # Lower weight (RÂ² = 0.598)\n    \"CNN-LSTM\": 0.07,  # Lowest weight (RÂ² = 0.621)\n}\n# NASA hour calibration (minimal - models already match well)\nNASA_HOUR_CALIBRATION = {\n    0: 0.0,\n    1: 0.0,",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "NASA_HOUR_CALIBRATION",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "NASA_HOUR_CALIBRATION = {\n    0: 0.0,\n    1: 0.0,\n    2: 0.0,\n    3: 0.0,\n    4: 0.0,\n    5: 0.0,  # Night\n    6: 0.0,  # Before sunrise\n    7: 1.0,  # Early morning - no calibration needed\n    8: 1.0,  # Morning",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "OPENMETEO_ENSEMBLE_WEIGHTS",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "OPENMETEO_ENSEMBLE_WEIGHTS = {\n    \"XGBoost\": 0.25,  # Equal weights\n    \"RandomForest\": 0.25,\n    \"LSTM\": 0.25,\n    \"CNN-LSTM\": 0.25,\n}\n# Open-Meteo hour calibration (corrects timing differences)\nOPENMETEO_HOUR_CALIBRATION = {\n    0: 0.0,\n    1: 0.0,",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "OPENMETEO_HOUR_CALIBRATION",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "OPENMETEO_HOUR_CALIBRATION = {\n    0: 0.0,\n    1: 0.0,\n    2: 0.0,\n    3: 0.0,\n    4: 0.0,\n    5: 0.0,  # Night\n    6: 0.0,  # Before sunrise\n    7: 0.0,  # Very early morning (API often shows 0)\n    8: 0.22,  # Early morning: significant scaling",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "MAX_LAG",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "MAX_LAG = 24\nSEQ_LEN = 24\n# ---------- 4. User input: day to forecast ----------\n# For API comparison, use today or a future date (within 16 days)\n# Open-Meteo only provides forecasts, not historical data\ntoday = datetime.now()\nyear = today.year\nmonth = today.month\nday = today.day\n# Override with specific date if needed (must be within forecast range for API)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "SEQ_LEN",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "SEQ_LEN = 24\n# ---------- 4. User input: day to forecast ----------\n# For API comparison, use today or a future date (within 16 days)\n# Open-Meteo only provides forecasts, not historical data\ntoday = datetime.now()\nyear = today.year\nmonth = today.month\nday = today.day\n# Override with specific date if needed (must be within forecast range for API)\n# For NASA POWER: Use historical dates (~1 week old or older)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "today",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "today = datetime.now()\nyear = today.year\nmonth = today.month\nday = today.day\n# Override with specific date if needed (must be within forecast range for API)\n# For NASA POWER: Use historical dates (~1 week old or older)\n# For Open-Meteo: Use today or future dates (up to 16 days)\nyear = 2025\nmonth = 12\nday = 21  # Current date for Open-Meteo forecast",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "year",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "year = today.year\nmonth = today.month\nday = today.day\n# Override with specific date if needed (must be within forecast range for API)\n# For NASA POWER: Use historical dates (~1 week old or older)\n# For Open-Meteo: Use today or future dates (up to 16 days)\nyear = 2025\nmonth = 12\nday = 21  # Current date for Open-Meteo forecast\nprint(f\"ðŸ“… Forecasting for: {year}-{month:02d}-{day:02d}\")",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "month",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "month = today.month\nday = today.day\n# Override with specific date if needed (must be within forecast range for API)\n# For NASA POWER: Use historical dates (~1 week old or older)\n# For Open-Meteo: Use today or future dates (up to 16 days)\nyear = 2025\nmonth = 12\nday = 21  # Current date for Open-Meteo forecast\nprint(f\"ðŸ“… Forecasting for: {year}-{month:02d}-{day:02d}\")\n# Calculate day of year for the forecast date",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "day",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "day = today.day\n# Override with specific date if needed (must be within forecast range for API)\n# For NASA POWER: Use historical dates (~1 week old or older)\n# For Open-Meteo: Use today or future dates (up to 16 days)\nyear = 2025\nmonth = 12\nday = 21  # Current date for Open-Meteo forecast\nprint(f\"ðŸ“… Forecasting for: {year}-{month:02d}-{day:02d}\")\n# Calculate day of year for the forecast date\nforecast_date = datetime(year, month, day)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "year",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "year = 2025\nmonth = 12\nday = 21  # Current date for Open-Meteo forecast\nprint(f\"ðŸ“… Forecasting for: {year}-{month:02d}-{day:02d}\")\n# Calculate day of year for the forecast date\nforecast_date = datetime(year, month, day)\nday_of_year = forecast_date.timetuple().tm_yday\n# Generate hourly datetime index for the day\ndate_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "month",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "month = 12\nday = 21  # Current date for Open-Meteo forecast\nprint(f\"ðŸ“… Forecasting for: {year}-{month:02d}-{day:02d}\")\n# Calculate day of year for the forecast date\nforecast_date = datetime(year, month, day)\nday_of_year = forecast_date.timetuple().tm_yday\n# Generate hourly datetime index for the day\ndate_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",\n    end=f\"{year}-{month:02d}-{day:02d} 23:00\",",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "day",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "day = 21  # Current date for Open-Meteo forecast\nprint(f\"ðŸ“… Forecasting for: {year}-{month:02d}-{day:02d}\")\n# Calculate day of year for the forecast date\nforecast_date = datetime(year, month, day)\nday_of_year = forecast_date.timetuple().tm_yday\n# Generate hourly datetime index for the day\ndate_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",\n    end=f\"{year}-{month:02d}-{day:02d} 23:00\",\n    freq=\"H\",",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "forecast_date",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "forecast_date = datetime(year, month, day)\nday_of_year = forecast_date.timetuple().tm_yday\n# Generate hourly datetime index for the day\ndate_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",\n    end=f\"{year}-{month:02d}-{day:02d} 23:00\",\n    freq=\"H\",\n)\nhours = date_index.hour\n# ---------- 5. Prepare tree-based model inputs ----------",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "day_of_year",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "day_of_year = forecast_date.timetuple().tm_yday\n# Generate hourly datetime index for the day\ndate_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",\n    end=f\"{year}-{month:02d}-{day:02d} 23:00\",\n    freq=\"H\",\n)\nhours = date_index.hour\n# ---------- 5. Prepare tree-based model inputs ----------\nlast_hist_values = df_day[target_col].values",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "date_index",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "date_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",\n    end=f\"{year}-{month:02d}-{day:02d} 23:00\",\n    freq=\"H\",\n)\nhours = date_index.hour\n# ---------- 5. Prepare tree-based model inputs ----------\nlast_hist_values = df_day[target_col].values\ntree_series = last_hist_values[-MAX_LAG:].tolist()  # last 24 hours\n# Use last known weather values",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "hours",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "hours = date_index.hour\n# ---------- 5. Prepare tree-based model inputs ----------\nlast_hist_values = df_day[target_col].values\ntree_series = last_hist_values[-MAX_LAG:].tolist()  # last 24 hours\n# Use last known weather values\nTemperature = df_day[\"Temperature\"].iloc[-1]\nHumiditySpecific = df_day[\"HumiditySpecific\"].iloc[-1]\nHumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "last_hist_values",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "last_hist_values = df_day[target_col].values\ntree_series = last_hist_values[-MAX_LAG:].tolist()  # last 24 hours\n# Use last known weather values\nTemperature = df_day[\"Temperature\"].iloc[-1]\nHumiditySpecific = df_day[\"HumiditySpecific\"].iloc[-1]\nHumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\n# Get typical SolarZenith and ClearSkyRadiation for each hour from historical data for this month",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "tree_series",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "tree_series = last_hist_values[-MAX_LAG:].tolist()  # last 24 hours\n# Use last known weather values\nTemperature = df_day[\"Temperature\"].iloc[-1]\nHumiditySpecific = df_day[\"HumiditySpecific\"].iloc[-1]\nHumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\n# Get typical SolarZenith and ClearSkyRadiation for each hour from historical data for this month\n# This is crucial: SolarZenith >= 90 means sun is below horizon (night)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "Temperature",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "Temperature = df_day[\"Temperature\"].iloc[-1]\nHumiditySpecific = df_day[\"HumiditySpecific\"].iloc[-1]\nHumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\n# Get typical SolarZenith and ClearSkyRadiation for each hour from historical data for this month\n# This is crucial: SolarZenith >= 90 means sun is below horizon (night)\nhourly_zenith = df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"SolarZenith\"].mean()\nhourly_clearsky = (",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "HumiditySpecific",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "HumiditySpecific = df_day[\"HumiditySpecific\"].iloc[-1]\nHumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\n# Get typical SolarZenith and ClearSkyRadiation for each hour from historical data for this month\n# This is crucial: SolarZenith >= 90 means sun is below horizon (night)\nhourly_zenith = df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"SolarZenith\"].mean()\nhourly_clearsky = (\n    df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"ClearSkyRadiation\"].mean()",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "HumidityRelative",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "HumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\n# Get typical SolarZenith and ClearSkyRadiation for each hour from historical data for this month\n# This is crucial: SolarZenith >= 90 means sun is below horizon (night)\nhourly_zenith = df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"SolarZenith\"].mean()\nhourly_clearsky = (\n    df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"ClearSkyRadiation\"].mean()\n)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "Pressure",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "Pressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\n# Get typical SolarZenith and ClearSkyRadiation for each hour from historical data for this month\n# This is crucial: SolarZenith >= 90 means sun is below horizon (night)\nhourly_zenith = df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"SolarZenith\"].mean()\nhourly_clearsky = (\n    df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"ClearSkyRadiation\"].mean()\n)\ntree_predictions = []",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "WindSpeed",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "WindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\n# Get typical SolarZenith and ClearSkyRadiation for each hour from historical data for this month\n# This is crucial: SolarZenith >= 90 means sun is below horizon (night)\nhourly_zenith = df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"SolarZenith\"].mean()\nhourly_clearsky = (\n    df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"ClearSkyRadiation\"].mean()\n)\ntree_predictions = []\nfor h in range(24):",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "WindDirection",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "WindDirection = df_day[\"WindDirection\"].iloc[-1]\n# Get typical SolarZenith and ClearSkyRadiation for each hour from historical data for this month\n# This is crucial: SolarZenith >= 90 means sun is below horizon (night)\nhourly_zenith = df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"SolarZenith\"].mean()\nhourly_clearsky = (\n    df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"ClearSkyRadiation\"].mean()\n)\ntree_predictions = []\nfor h in range(24):\n    hour = hours[h]",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "hourly_zenith",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "hourly_zenith = df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"SolarZenith\"].mean()\nhourly_clearsky = (\n    df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"ClearSkyRadiation\"].mean()\n)\ntree_predictions = []\nfor h in range(24):\n    hour = hours[h]\n    # Compute cyclical features for current hour\n    hour_sin = np.sin(2 * np.pi * hour / 24)\n    hour_cos = np.cos(2 * np.pi * hour / 24)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "hourly_clearsky",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "hourly_clearsky = (\n    df_day[df_day[\"month\"] == month].groupby(\"hour\")[\"ClearSkyRadiation\"].mean()\n)\ntree_predictions = []\nfor h in range(24):\n    hour = hours[h]\n    # Compute cyclical features for current hour\n    hour_sin = np.sin(2 * np.pi * hour / 24)\n    hour_cos = np.cos(2 * np.pi * hour / 24)\n    month_sin = np.sin(2 * np.pi * month / 12)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "tree_predictions",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "tree_predictions = []\nfor h in range(24):\n    hour = hours[h]\n    # Compute cyclical features for current hour\n    hour_sin = np.sin(2 * np.pi * hour / 24)\n    hour_cos = np.cos(2 * np.pi * hour / 24)\n    month_sin = np.sin(2 * np.pi * month / 12)\n    month_cos = np.cos(2 * np.pi * month / 12)\n    doy_sin = np.sin(2 * np.pi * day_of_year / 365)\n    doy_cos = np.cos(2 * np.pi * day_of_year / 365)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "seq_features",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "seq_features = [\n    \"hour\",\n    \"month\",\n    \"day_of_year\",\n    \"hour_sin\",\n    \"hour_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"doy_sin\",\n    \"doy_cos\",",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "seq_data",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "seq_data = df_day[seq_features].copy()\n# Scale features (excluding target which is last column)\nX_seq_scaled = scaler_X.transform(seq_data.drop(columns=[target_col])).astype(\n    np.float32\n)\n# Start with last SEQ_LEN rows\nseq_array = X_seq_scaled[-SEQ_LEN:].copy()\nseq_predictions = []\nfor h in range(24):\n    X_seq_input = seq_array[-SEQ_LEN:].reshape(1, SEQ_LEN, seq_array.shape[1])",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "X_seq_scaled",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "X_seq_scaled = scaler_X.transform(seq_data.drop(columns=[target_col])).astype(\n    np.float32\n)\n# Start with last SEQ_LEN rows\nseq_array = X_seq_scaled[-SEQ_LEN:].copy()\nseq_predictions = []\nfor h in range(24):\n    X_seq_input = seq_array[-SEQ_LEN:].reshape(1, SEQ_LEN, seq_array.shape[1])\n    lstm_pred_scaled = lstm_model.predict(X_seq_input, verbose=0)\n    cnn_pred_scaled = cnn_lstm_model.predict(X_seq_input, verbose=0)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "seq_array",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "seq_array = X_seq_scaled[-SEQ_LEN:].copy()\nseq_predictions = []\nfor h in range(24):\n    X_seq_input = seq_array[-SEQ_LEN:].reshape(1, SEQ_LEN, seq_array.shape[1])\n    lstm_pred_scaled = lstm_model.predict(X_seq_input, verbose=0)\n    cnn_pred_scaled = cnn_lstm_model.predict(X_seq_input, verbose=0)\n    lstm_pred = scaler_y.inverse_transform(lstm_pred_scaled.reshape(-1, 1))[0, 0]\n    cnn_pred = scaler_y.inverse_transform(cnn_pred_scaled.reshape(-1, 1))[0, 0]\n    # Ensure non-negative predictions\n    lstm_pred = max(0, lstm_pred)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "seq_predictions",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "seq_predictions = []\nfor h in range(24):\n    X_seq_input = seq_array[-SEQ_LEN:].reshape(1, SEQ_LEN, seq_array.shape[1])\n    lstm_pred_scaled = lstm_model.predict(X_seq_input, verbose=0)\n    cnn_pred_scaled = cnn_lstm_model.predict(X_seq_input, verbose=0)\n    lstm_pred = scaler_y.inverse_transform(lstm_pred_scaled.reshape(-1, 1))[0, 0]\n    cnn_pred = scaler_y.inverse_transform(cnn_pred_scaled.reshape(-1, 1))[0, 0]\n    # Ensure non-negative predictions\n    lstm_pred = max(0, lstm_pred)\n    cnn_pred = max(0, cnn_pred)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "xgb_preds",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "xgb_preds = [x for x, _ in tree_predictions]\nrf_preds = [x for _, x in tree_predictions]\nlstm_preds = [x for x, _ in seq_predictions]\ncnn_preds = [x for _, x in seq_predictions]\n# Select ensemble configuration based on API\nprint(f\"\\nðŸ”§ Using ensemble configuration for: {SELECTED_API.upper()}\")\nif SELECTED_API.lower() == \"nasa-power\":\n    ACTIVE_WEIGHTS = NASA_ENSEMBLE_WEIGHTS\n    HOUR_CALIBRATION = NASA_HOUR_CALIBRATION\n    print(",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "rf_preds",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "rf_preds = [x for _, x in tree_predictions]\nlstm_preds = [x for x, _ in seq_predictions]\ncnn_preds = [x for _, x in seq_predictions]\n# Select ensemble configuration based on API\nprint(f\"\\nðŸ”§ Using ensemble configuration for: {SELECTED_API.upper()}\")\nif SELECTED_API.lower() == \"nasa-power\":\n    ACTIVE_WEIGHTS = NASA_ENSEMBLE_WEIGHTS\n    HOUR_CALIBRATION = NASA_HOUR_CALIBRATION\n    print(\n        f\"   Weights: XGB={ACTIVE_WEIGHTS['XGBoost']:.0%}, RF={ACTIVE_WEIGHTS['RandomForest']:.0%}, LSTM={ACTIVE_WEIGHTS['LSTM']:.0%}, CNN={ACTIVE_WEIGHTS['CNN-LSTM']:.0%}\"",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "lstm_preds",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "lstm_preds = [x for x, _ in seq_predictions]\ncnn_preds = [x for _, x in seq_predictions]\n# Select ensemble configuration based on API\nprint(f\"\\nðŸ”§ Using ensemble configuration for: {SELECTED_API.upper()}\")\nif SELECTED_API.lower() == \"nasa-power\":\n    ACTIVE_WEIGHTS = NASA_ENSEMBLE_WEIGHTS\n    HOUR_CALIBRATION = NASA_HOUR_CALIBRATION\n    print(\n        f\"   Weights: XGB={ACTIVE_WEIGHTS['XGBoost']:.0%}, RF={ACTIVE_WEIGHTS['RandomForest']:.0%}, LSTM={ACTIVE_WEIGHTS['LSTM']:.0%}, CNN={ACTIVE_WEIGHTS['CNN-LSTM']:.0%}\"\n    )",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "cnn_preds",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "cnn_preds = [x for _, x in seq_predictions]\n# Select ensemble configuration based on API\nprint(f\"\\nðŸ”§ Using ensemble configuration for: {SELECTED_API.upper()}\")\nif SELECTED_API.lower() == \"nasa-power\":\n    ACTIVE_WEIGHTS = NASA_ENSEMBLE_WEIGHTS\n    HOUR_CALIBRATION = NASA_HOUR_CALIBRATION\n    print(\n        f\"   Weights: XGB={ACTIVE_WEIGHTS['XGBoost']:.0%}, RF={ACTIVE_WEIGHTS['RandomForest']:.0%}, LSTM={ACTIVE_WEIGHTS['LSTM']:.0%}, CNN={ACTIVE_WEIGHTS['CNN-LSTM']:.0%}\"\n    )\n    print(\"   Calibration: Minimal (models match NASA data well)\")",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "ensemble_preds",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "ensemble_preds = []\nfor i in range(24):\n    hour = hours[i]\n    # Get predictions\n    xgb_val = xgb_preds[i]\n    rf_val = rf_preds[i]\n    lstm_val = lstm_preds[i]\n    cnn_val = cnn_preds[i]\n    # Calculate weighted ensemble based on selected API configuration\n    base_ensemble = (",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "results_day",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "results_day = pd.DataFrame(\n    {\n        \"datetime\": date_index,\n        \"XGBoost\": xgb_preds,\n        \"RandomForest\": rf_preds,\n        \"LSTM\": lstm_preds,\n        \"CNN_LSTM\": cnn_preds,\n        \"Ensemble\": ensemble_preds,\n        f\"{api_name}_API\": api_preds,\n    }",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "api_col_name",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "api_col_name = api_name[:10]  # Truncate for formatting\nprint(\n    f\"{'Hour':>4} | {'XGBoost':>10} | {'RandomForest':>13} | {'LSTM':>8} | {'CNN-LSTM':>10} | {'Ensemble':>10} | {api_col_name:>10}\"\n)\nprint(\"-\" * 85)\nfor i, row in results_day.iterrows():\n    hour = row[\"datetime\"].hour\n    api_val = row.get(f\"{api_name}_API\", np.nan)\n    api_str = f\"{api_val:10.2f}\" if not np.isnan(api_val) else \"       N/A\"\n    print(",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "ax1",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "ax1 = axes[0, 0]\nax1.plot(\n    results_day[\"datetime\"].dt.hour,\n    results_day[\"XGBoost\"],\n    label=\"XGBoost\",\n    marker=\"o\",\n    markersize=3,\n)\nax1.plot(\n    results_day[\"datetime\"].dt.hour,",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "ax2",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "ax2 = axes[0, 1]\nax2.plot(\n    range(24),\n    ensemble_preds,\n    label=\"Ensemble (Our Model)\",\n    linewidth=2.5,\n    color=\"blue\",\n    marker=\"o\",\n    markersize=5,\n)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "ax3",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "ax3 = axes[1, 0]\n# Create calibrated range using tree models with calibration factors\ncalibrated_xgb = []\ncalibrated_rf = []\nfor i in range(24):\n    hour = hours[i]\n    cal_factor = HOUR_CALIBRATION.get(hour, 1.0)\n    calibrated_xgb.append(min(max(0, xgb_preds[i] * cal_factor), 1000))\n    calibrated_rf.append(min(max(0, rf_preds[i] * cal_factor), 1000))\n# Use calibrated tree models for the range (Â±10% uncertainty band)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "calibrated_xgb",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "calibrated_xgb = []\ncalibrated_rf = []\nfor i in range(24):\n    hour = hours[i]\n    cal_factor = HOUR_CALIBRATION.get(hour, 1.0)\n    calibrated_xgb.append(min(max(0, xgb_preds[i] * cal_factor), 1000))\n    calibrated_rf.append(min(max(0, rf_preds[i] * cal_factor), 1000))\n# Use calibrated tree models for the range (Â±10% uncertainty band)\npred_center = np.array(ensemble_preds)\npred_min = pred_center * 0.85  # Lower bound (-15%)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "calibrated_rf",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "calibrated_rf = []\nfor i in range(24):\n    hour = hours[i]\n    cal_factor = HOUR_CALIBRATION.get(hour, 1.0)\n    calibrated_xgb.append(min(max(0, xgb_preds[i] * cal_factor), 1000))\n    calibrated_rf.append(min(max(0, rf_preds[i] * cal_factor), 1000))\n# Use calibrated tree models for the range (Â±10% uncertainty band)\npred_center = np.array(ensemble_preds)\npred_min = pred_center * 0.85  # Lower bound (-15%)\npred_max = pred_center * 1.15  # Upper bound (+15%)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "pred_center",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "pred_center = np.array(ensemble_preds)\npred_min = pred_center * 0.85  # Lower bound (-15%)\npred_max = pred_center * 1.15  # Upper bound (+15%)\nax3.fill_between(\n    range(24),\n    pred_min,\n    pred_max,\n    alpha=0.3,\n    color=\"blue\",\n    label=\"Calibrated Range (Â±15%)\",",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "pred_min",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "pred_min = pred_center * 0.85  # Lower bound (-15%)\npred_max = pred_center * 1.15  # Upper bound (+15%)\nax3.fill_between(\n    range(24),\n    pred_min,\n    pred_max,\n    alpha=0.3,\n    color=\"blue\",\n    label=\"Calibrated Range (Â±15%)\",\n)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "pred_max",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "pred_max = pred_center * 1.15  # Upper bound (+15%)\nax3.fill_between(\n    range(24),\n    pred_min,\n    pred_max,\n    alpha=0.3,\n    color=\"blue\",\n    label=\"Calibrated Range (Â±15%)\",\n)\nax3.plot(",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "ax4",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "ax4 = axes[1, 1]\nif not all(np.isnan(api_preds)):\n    diff = np.array(ensemble_preds) - np.array(api_preds)\n    colors = [\"green\" if d >= 0 else \"red\" for d in diff]\n    ax4.bar(range(24), diff, color=colors, alpha=0.7, edgecolor=\"black\")\n    ax4.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=1)\n    ax4.set_xlabel(\"Hour of Day\")\n    ax4.set_ylabel(\"Difference (W/mÂ²)\")\n    ax4.set_title(f\"ðŸ“Š Prediction Difference (Ensemble - {api_name})\")\n    ax4.set_xticks(range(0, 24, 2))",
        "detail": "trend",
        "documentation": {}
    }
]