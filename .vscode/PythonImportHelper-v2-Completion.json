[
    {
        "label": "openmeteo_requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openmeteo_requests",
        "description": "openmeteo_requests",
        "detail": "openmeteo_requests",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "requests_cache",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests_cache",
        "description": "requests_cache",
        "detail": "requests_cache",
        "documentation": {}
    },
    {
        "label": "retry",
        "importPath": "retry_requests",
        "description": "retry_requests",
        "isExtraImport": true,
        "detail": "retry_requests",
        "documentation": {}
    },
    {
        "label": "streamlit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "streamlit",
        "description": "streamlit",
        "detail": "streamlit",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "joblib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "joblib",
        "description": "joblib",
        "detail": "joblib",
        "documentation": {}
    },
    {
        "label": "mean_absolute_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "r2_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "xgboost",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xgboost",
        "description": "xgboost",
        "detail": "xgboost",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "layers",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "callbacks",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "backend",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "tensorflow.keras.models",
        "description": "tensorflow.keras.models",
        "isExtraImport": true,
        "detail": "tensorflow.keras.models",
        "documentation": {}
    },
    {
        "label": "cache_session",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\nretry_session = retry(cache_session, retries=5, backoff_factor=0.2)\nopenmeteo = openmeteo_requests.Client(session=retry_session)\n# Make sure all required weather variables are listed here\nurl = \"https://historical-forecast-api.open-meteo.com/v1/forecast\"\nparams = {\n    \"latitude\": 33.7215,\n    \"longitude\": 73.0433,\n    \"start_date\": \"2016-01-01\",\n    \"end_date\": \"2025-10-28\",",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "retry_session",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\nopenmeteo = openmeteo_requests.Client(session=retry_session)\n# Make sure all required weather variables are listed here\nurl = \"https://historical-forecast-api.open-meteo.com/v1/forecast\"\nparams = {\n    \"latitude\": 33.7215,\n    \"longitude\": 73.0433,\n    \"start_date\": \"2016-01-01\",\n    \"end_date\": \"2025-10-28\",\n    \"daily\": [\"sunset\", \"uv_index_max\", \"apparent_temperature_max\", \"snowfall_sum\"],",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "openmeteo",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "openmeteo = openmeteo_requests.Client(session=retry_session)\n# Make sure all required weather variables are listed here\nurl = \"https://historical-forecast-api.open-meteo.com/v1/forecast\"\nparams = {\n    \"latitude\": 33.7215,\n    \"longitude\": 73.0433,\n    \"start_date\": \"2016-01-01\",\n    \"end_date\": \"2025-10-28\",\n    \"daily\": [\"sunset\", \"uv_index_max\", \"apparent_temperature_max\", \"snowfall_sum\"],\n    \"hourly\": [\"temperature_2m\", \"uv_index_clear_sky\", \"direct_radiation\", \"direct_normal_irradiance\"],",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "url",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "url = \"https://historical-forecast-api.open-meteo.com/v1/forecast\"\nparams = {\n    \"latitude\": 33.7215,\n    \"longitude\": 73.0433,\n    \"start_date\": \"2016-01-01\",\n    \"end_date\": \"2025-10-28\",\n    \"daily\": [\"sunset\", \"uv_index_max\", \"apparent_temperature_max\", \"snowfall_sum\"],\n    \"hourly\": [\"temperature_2m\", \"uv_index_clear_sky\", \"direct_radiation\", \"direct_normal_irradiance\"],\n    \"timezone\": \"Asia/Bangkok\",\n}",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "params",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "params = {\n    \"latitude\": 33.7215,\n    \"longitude\": 73.0433,\n    \"start_date\": \"2016-01-01\",\n    \"end_date\": \"2025-10-28\",\n    \"daily\": [\"sunset\", \"uv_index_max\", \"apparent_temperature_max\", \"snowfall_sum\"],\n    \"hourly\": [\"temperature_2m\", \"uv_index_clear_sky\", \"direct_radiation\", \"direct_normal_irradiance\"],\n    \"timezone\": \"Asia/Bangkok\",\n}\n# API request",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "responses",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "responses = openmeteo.weather_api(url, params=params)\nresponse = responses[0]\nprint(f\"Coordinates: {response.Latitude()}Â°N {response.Longitude()}Â°E\")\nprint(f\"Elevation: {response.Elevation()} m asl\")\nprint(f\"Timezone: {response.Timezone()}{response.TimezoneAbbreviation()}\")\nprint(f\"Timezone difference to GMT+0: {response.UtcOffsetSeconds()}s\")\n# Process hourly data\nhourly = response.Hourly()\nhourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\nhourly_uv_index_clear_sky = hourly.Variables(1).ValuesAsNumpy()",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "response = responses[0]\nprint(f\"Coordinates: {response.Latitude()}Â°N {response.Longitude()}Â°E\")\nprint(f\"Elevation: {response.Elevation()} m asl\")\nprint(f\"Timezone: {response.Timezone()}{response.TimezoneAbbreviation()}\")\nprint(f\"Timezone difference to GMT+0: {response.UtcOffsetSeconds()}s\")\n# Process hourly data\nhourly = response.Hourly()\nhourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\nhourly_uv_index_clear_sky = hourly.Variables(1).ValuesAsNumpy()\nhourly_direct_radiation = hourly.Variables(2).ValuesAsNumpy()",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly = response.Hourly()\nhourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\nhourly_uv_index_clear_sky = hourly.Variables(1).ValuesAsNumpy()\nhourly_direct_radiation = hourly.Variables(2).ValuesAsNumpy()\nhourly_direct_normal_irradiance = hourly.Variables(3).ValuesAsNumpy()\nhourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_temperature_2m",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\nhourly_uv_index_clear_sky = hourly.Variables(1).ValuesAsNumpy()\nhourly_direct_radiation = hourly.Variables(2).ValuesAsNumpy()\nhourly_direct_normal_irradiance = hourly.Variables(3).ValuesAsNumpy()\nhourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),\n        inclusive=\"left\"",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_uv_index_clear_sky",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_uv_index_clear_sky = hourly.Variables(1).ValuesAsNumpy()\nhourly_direct_radiation = hourly.Variables(2).ValuesAsNumpy()\nhourly_direct_normal_irradiance = hourly.Variables(3).ValuesAsNumpy()\nhourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),\n        inclusive=\"left\"\n    ),",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_direct_radiation",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_direct_radiation = hourly.Variables(2).ValuesAsNumpy()\nhourly_direct_normal_irradiance = hourly.Variables(3).ValuesAsNumpy()\nhourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),\n        inclusive=\"left\"\n    ),\n    \"temperature_2m\": hourly_temperature_2m,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_direct_normal_irradiance",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_direct_normal_irradiance = hourly.Variables(3).ValuesAsNumpy()\nhourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),\n        inclusive=\"left\"\n    ),\n    \"temperature_2m\": hourly_temperature_2m,\n    \"uv_index_clear_sky\": hourly_uv_index_clear_sky,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_data",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=hourly.Interval()),\n        inclusive=\"left\"\n    ),\n    \"temperature_2m\": hourly_temperature_2m,\n    \"uv_index_clear_sky\": hourly_uv_index_clear_sky,\n    \"direct_radiation\": hourly_direct_radiation,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "hourly_dataframe",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "hourly_dataframe = pd.DataFrame(data=hourly_data)\nprint(\"\\nHourly data\\n\", hourly_dataframe.head())\n# Process daily data\ndaily = response.Daily()\ndaily_sunset = daily.Variables(0).ValuesInt64AsNumpy()\ndaily_uv_index_max = daily.Variables(1).ValuesAsNumpy()\ndaily_apparent_temperature_max = daily.Variables(2).ValuesAsNumpy()\ndaily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily = response.Daily()\ndaily_sunset = daily.Variables(0).ValuesInt64AsNumpy()\ndaily_uv_index_max = daily.Variables(1).ValuesAsNumpy()\ndaily_apparent_temperature_max = daily.Variables(2).ValuesAsNumpy()\ndaily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_sunset",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_sunset = daily.Variables(0).ValuesInt64AsNumpy()\ndaily_uv_index_max = daily.Variables(1).ValuesAsNumpy()\ndaily_apparent_temperature_max = daily.Variables(2).ValuesAsNumpy()\ndaily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),\n        inclusive=\"left\"",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_uv_index_max",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_uv_index_max = daily.Variables(1).ValuesAsNumpy()\ndaily_apparent_temperature_max = daily.Variables(2).ValuesAsNumpy()\ndaily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),\n        inclusive=\"left\"\n    ),",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_apparent_temperature_max",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_apparent_temperature_max = daily.Variables(2).ValuesAsNumpy()\ndaily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),\n        inclusive=\"left\"\n    ),\n    \"sunset\": daily_sunset,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_snowfall_sum",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_snowfall_sum = daily.Variables(3).ValuesAsNumpy()\ndaily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),\n        inclusive=\"left\"\n    ),\n    \"sunset\": daily_sunset,\n    \"uv_index_max\": daily_uv_index_max,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_data",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_data = {\n    \"date\": pd.date_range(\n        start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n        end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n        freq=pd.Timedelta(seconds=daily.Interval()),\n        inclusive=\"left\"\n    ),\n    \"sunset\": daily_sunset,\n    \"uv_index_max\": daily_uv_index_max,\n    \"apparent_temperature_max\": daily_apparent_temperature_max,",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "daily_dataframe",
        "kind": 5,
        "importPath": "NASA meteriological and solar radiaton data.API_call",
        "description": "NASA meteriological and solar radiaton data.API_call",
        "peekOfCode": "daily_dataframe = pd.DataFrame(data=daily_data)\nprint(\"\\nDaily data\\n\", daily_dataframe.head())\n# ðŸ’¾ Save data to CSV files\nhourly_dataframe.to_csv(\"hourly_weather_data.csv\", index=False)\ndaily_dataframe.to_csv(\"daily_weather_data.csv\", index=False)\nprint(\"\\nData saved successfully:\")\nprint(\" - hourly_weather_data.csv\")\nprint(\" - daily_weather_data.csv\")",
        "detail": "NASA meteriological and solar radiaton data.API_call",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\ntry:\n    from tensorflow.keras.models import load_model\n    TENSORFLOW_AVAILABLE = True\nexcept:\n    TENSORFLOW_AVAILABLE = False\ntry:\n    import xgboost as xgb\n    XGBOOST_AVAILABLE = True\nexcept:",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "selected_date",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "selected_date = st.sidebar.date_input(\"Forecast Date\", datetime.date.today())\nselected_time = st.sidebar.time_input(\"Forecast Time\", datetime.datetime.now().time())\napi_choice = st.sidebar.selectbox(\"API Source\", [\"NASA\", \"Local Weather\", \"Custom API\"])\n# --- Model selection ---\nmodel_folder = \"save_model\"\navailable_models = [\n    (\"LSTM\", \"lstm_model.h5\"),\n    (\"CNN-LSTM\", \"cnn_lstm_model.h5\"),\n    (\"Random Forest\", \"random_forest_model.pkl\"),\n    (\"XGBoost\", \"xgboost_model.pkl\"),",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "selected_time",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "selected_time = st.sidebar.time_input(\"Forecast Time\", datetime.datetime.now().time())\napi_choice = st.sidebar.selectbox(\"API Source\", [\"NASA\", \"Local Weather\", \"Custom API\"])\n# --- Model selection ---\nmodel_folder = \"save_model\"\navailable_models = [\n    (\"LSTM\", \"lstm_model.h5\"),\n    (\"CNN-LSTM\", \"cnn_lstm_model.h5\"),\n    (\"Random Forest\", \"random_forest_model.pkl\"),\n    (\"XGBoost\", \"xgboost_model.pkl\"),\n]",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "api_choice",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "api_choice = st.sidebar.selectbox(\"API Source\", [\"NASA\", \"Local Weather\", \"Custom API\"])\n# --- Model selection ---\nmodel_folder = \"save_model\"\navailable_models = [\n    (\"LSTM\", \"lstm_model.h5\"),\n    (\"CNN-LSTM\", \"cnn_lstm_model.h5\"),\n    (\"Random Forest\", \"random_forest_model.pkl\"),\n    (\"XGBoost\", \"xgboost_model.pkl\"),\n]\n# Check if files exist",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "model_folder",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "model_folder = \"save_model\"\navailable_models = [\n    (\"LSTM\", \"lstm_model.h5\"),\n    (\"CNN-LSTM\", \"cnn_lstm_model.h5\"),\n    (\"Random Forest\", \"random_forest_model.pkl\"),\n    (\"XGBoost\", \"xgboost_model.pkl\"),\n]\n# Check if files exist\nmodel_files = [\n    m for m in available_models if os.path.exists(os.path.join(model_folder, m[1]))",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "available_models",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "available_models = [\n    (\"LSTM\", \"lstm_model.h5\"),\n    (\"CNN-LSTM\", \"cnn_lstm_model.h5\"),\n    (\"Random Forest\", \"random_forest_model.pkl\"),\n    (\"XGBoost\", \"xgboost_model.pkl\"),\n]\n# Check if files exist\nmodel_files = [\n    m for m in available_models if os.path.exists(os.path.join(model_folder, m[1]))\n]",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "model_files",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "model_files = [\n    m for m in available_models if os.path.exists(os.path.join(model_folder, m[1]))\n]\nif not model_files:\n    model_files = available_models\nmodel_names = [m[0] for m in model_files]\nselected_models = st.sidebar.multiselect(\n    \"Select Models\", model_names, default=model_names\n)\nst.sidebar.markdown(\"---\")",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "model_names",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "model_names = [m[0] for m in model_files]\nselected_models = st.sidebar.multiselect(\n    \"Select Models\", model_names, default=model_names\n)\nst.sidebar.markdown(\"---\")\nst.sidebar.write(f\"**Date:** {selected_date}\")\nst.sidebar.write(f\"**Time:** {selected_time}\")\nst.sidebar.write(\n    f\"**Models:** {', '.join(selected_models) if selected_models else 'None'}\"\n)",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "selected_models",
        "kind": 5,
        "importPath": "frontend",
        "description": "frontend",
        "peekOfCode": "selected_models = st.sidebar.multiselect(\n    \"Select Models\", model_names, default=model_names\n)\nst.sidebar.markdown(\"---\")\nst.sidebar.write(f\"**Date:** {selected_date}\")\nst.sidebar.write(f\"**Time:** {selected_time}\")\nst.sidebar.write(\n    f\"**Models:** {', '.join(selected_models) if selected_models else 'None'}\"\n)\n# --- Main ---",
        "detail": "frontend",
        "documentation": {}
    },
    {
        "label": "rmse",
        "kind": 2,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "def rmse(a, b):\n    return np.sqrt(mean_squared_error(a, b))\n# ---------- 1. Load data ----------\ndf = pd.read_csv(\"NASA meteriological and solar radiaton data/lahore_hourly_filled.csv\")\ndf.columns = df.columns.str.strip()\nif \"datetime\" not in df.columns:\n    raise ValueError(f\"'datetime' column not found\")\ndf[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"datetime\"])\ndf.set_index(\"datetime\", inplace=True)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "make_lag_features",
        "kind": 2,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "def make_lag_features(series_df, max_lag=24):\n    # Include cyclical hour/month encoding for better periodicity learning\n    cols_base = [\n        target_col,\n        \"hour\",\n        \"month\",\n        \"hour_sin\",\n        \"hour_cos\",\n        \"month_sin\",\n        \"month_cos\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "create_sequences",
        "kind": 2,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "def create_sequences(df_in, seq_len=24):\n    Xs, ys, idxs = [], [], []\n    X_arr = scaler_X.transform(df_in.drop(columns=[target_col]))\n    y_arr = scaler_y.transform(df_in[[target_col]]).flatten()\n    for i in range(seq_len, len(df_in)):\n        Xs.append(X_arr[i - seq_len : i])\n        ys.append(y_arr[i])\n        idxs.append(df_in.index[i])\n    return np.array(Xs), np.array(ys), np.array(idxs)\nX_seq_all, y_seq_all, idxs_all = create_sequences(df_seq)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport joblib\nimport tensorflow as tf",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "RANDOM_STATE",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "RANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntf.random.set_seed(RANDOM_STATE)\n# create save directory\nos.makedirs(\"saved_models\", exist_ok=True)\n# TF GPU memory growth (Colab safe)\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\nif gpus:\n    try:\n        for gpu in gpus:",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "gpus",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except Exception:\n        pass\ndef rmse(a, b):\n    return np.sqrt(mean_squared_error(a, b))\n# ---------- 1. Load data ----------",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df = pd.read_csv(\"NASA meteriological and solar radiaton data/lahore_hourly_filled.csv\")\ndf.columns = df.columns.str.strip()\nif \"datetime\" not in df.columns:\n    raise ValueError(f\"'datetime' column not found\")\ndf[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"datetime\"])\ndf.set_index(\"datetime\", inplace=True)\ndf = df.sort_index()\ndf = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df.columns",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df.columns = df.columns.str.strip()\nif \"datetime\" not in df.columns:\n    raise ValueError(f\"'datetime' column not found\")\ndf[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"datetime\"])\ndf.set_index(\"datetime\", inplace=True)\ndf = df.sort_index()\ndf = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())\nrequired = [",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df[\"datetime\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"datetime\"])\ndf.set_index(\"datetime\", inplace=True)\ndf = df.sort_index()\ndf = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())\nrequired = [\n    \"ClearSkyRadiation\",\n    \"SolarRadiation\",\n    \"DirectRadiation\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df = df.dropna(subset=[\"datetime\"])\ndf.set_index(\"datetime\", inplace=True)\ndf = df.sort_index()\ndf = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())\nrequired = [\n    \"ClearSkyRadiation\",\n    \"SolarRadiation\",\n    \"DirectRadiation\",\n    \"DiffuseRadiation\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df = df.sort_index()\ndf = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())\nrequired = [\n    \"ClearSkyRadiation\",\n    \"SolarRadiation\",\n    \"DirectRadiation\",\n    \"DiffuseRadiation\",\n    \"SolarZenith\",\n    \"Temperature\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df = df.apply(pd.to_numeric, errors=\"coerce\")\nprint(\"Loaded CSV range:\", df.index.min(), \"â†’\", df.index.max())\nrequired = [\n    \"ClearSkyRadiation\",\n    \"SolarRadiation\",\n    \"DirectRadiation\",\n    \"DiffuseRadiation\",\n    \"SolarZenith\",\n    \"Temperature\",\n    \"HumiditySpecific\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "required",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "required = [\n    \"ClearSkyRadiation\",\n    \"SolarRadiation\",\n    \"DirectRadiation\",\n    \"DiffuseRadiation\",\n    \"SolarZenith\",\n    \"Temperature\",\n    \"HumiditySpecific\",\n    \"HumidityRelative\",\n    \"Pressure\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "missing",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "missing = [c for c in required if c not in df.columns]\nif missing:\n    raise ValueError(\"Missing columns: \", missing)\n# IMPORTANT: Use ALL hours (not just daylight) so model learns night = 0\ndf_day = df.copy()\ndf_day.dropna(subset=[\"SolarRadiation\"], inplace=True)\n# Add cyclical encoding for hour and month (helps model understand periodicity)\ndf_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day = df.copy()\ndf_day.dropna(subset=[\"SolarRadiation\"], inplace=True)\n# Add cyclical encoding for hour and month (helps model understand periodicity)\ndf_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\nprint(\"Total records (including night):\", len(df_day))\n# ---------- 2. Train/Test split ----------\nsplit_idx = int(len(df_day) * 0.8)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"month\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"month\"] = df_day.index.month\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\nprint(\"Total records (including night):\", len(df_day))\n# ---------- 2. Train/Test split ----------\nsplit_idx = int(len(df_day) * 0.8)\ntrain = df_day.iloc[:split_idx].copy()",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour_sin\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\nprint(\"Total records (including night):\", len(df_day))\n# ---------- 2. Train/Test split ----------\nsplit_idx = int(len(df_day) * 0.8)\ntrain = df_day.iloc[:split_idx].copy()\ntest = df_day.iloc[split_idx:].copy()",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour_cos\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\nprint(\"Total records (including night):\", len(df_day))\n# ---------- 2. Train/Test split ----------\nsplit_idx = int(len(df_day) * 0.8)\ntrain = df_day.iloc[:split_idx].copy()\ntest = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"month_sin\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\nprint(\"Total records (including night):\", len(df_day))\n# ---------- 2. Train/Test split ----------\nsplit_idx = int(len(df_day) * 0.8)\ntrain = df_day.iloc[:split_idx].copy()\ntest = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------\nMAX_LAG = 24",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_day[\"month_cos\"]",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\nprint(\"Total records (including night):\", len(df_day))\n# ---------- 2. Train/Test split ----------\nsplit_idx = int(len(df_day) * 0.8)\ntrain = df_day.iloc[:split_idx].copy()\ntest = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------\nMAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "target_col",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "target_col = \"SolarRadiation\"\nprint(\"Total records (including night):\", len(df_day))\n# ---------- 2. Train/Test split ----------\nsplit_idx = int(len(df_day) * 0.8)\ntrain = df_day.iloc[:split_idx].copy()\ntest = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------\nMAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):\n    # Include cyclical hour/month encoding for better periodicity learning",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "split_idx",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "split_idx = int(len(df_day) * 0.8)\ntrain = df_day.iloc[:split_idx].copy()\ntest = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------\nMAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):\n    # Include cyclical hour/month encoding for better periodicity learning\n    cols_base = [\n        target_col,\n        \"hour\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "train = df_day.iloc[:split_idx].copy()\ntest = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------\nMAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):\n    # Include cyclical hour/month encoding for better periodicity learning\n    cols_base = [\n        target_col,\n        \"hour\",\n        \"month\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "test = df_day.iloc[split_idx:].copy()\n# ---------- 3. Lag features ----------\nMAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):\n    # Include cyclical hour/month encoding for better periodicity learning\n    cols_base = [\n        target_col,\n        \"hour\",\n        \"month\",\n        \"hour_sin\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "MAX_LAG",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "MAX_LAG = 24\ndef make_lag_features(series_df, max_lag=24):\n    # Include cyclical hour/month encoding for better periodicity learning\n    cols_base = [\n        target_col,\n        \"hour\",\n        \"month\",\n        \"hour_sin\",\n        \"hour_cos\",\n        \"month_sin\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_lag",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_lag = make_lag_features(df_day, MAX_LAG)\ntrain_lag = df_lag.loc[df_lag.index.intersection(train.index)]\ntest_lag = df_lag.loc[df_lag.index.intersection(test.index)]\n# fallback if no test rows\nif len(test_lag) == 0:\n    train_lag = df_lag.iloc[:-MAX_LAG].copy()\n    test_lag = df_lag.iloc[-MAX_LAG:].copy()\nX_train_tree = train_lag.drop(columns=[target_col])\ny_train_tree = train_lag[target_col]\nX_test_tree = test_lag.drop(columns=[target_col])",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "train_lag",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "train_lag = df_lag.loc[df_lag.index.intersection(train.index)]\ntest_lag = df_lag.loc[df_lag.index.intersection(test.index)]\n# fallback if no test rows\nif len(test_lag) == 0:\n    train_lag = df_lag.iloc[:-MAX_LAG].copy()\n    test_lag = df_lag.iloc[-MAX_LAG:].copy()\nX_train_tree = train_lag.drop(columns=[target_col])\ny_train_tree = train_lag[target_col]\nX_test_tree = test_lag.drop(columns=[target_col])\ny_test = test_lag[target_col]",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "test_lag",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "test_lag = df_lag.loc[df_lag.index.intersection(test.index)]\n# fallback if no test rows\nif len(test_lag) == 0:\n    train_lag = df_lag.iloc[:-MAX_LAG].copy()\n    test_lag = df_lag.iloc[-MAX_LAG:].copy()\nX_train_tree = train_lag.drop(columns=[target_col])\ny_train_tree = train_lag[target_col]\nX_test_tree = test_lag.drop(columns=[target_col])\ny_test = test_lag[target_col]\n# ---------- 4. Scaling for deep learning ----------",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "X_train_tree",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "X_train_tree = train_lag.drop(columns=[target_col])\ny_train_tree = train_lag[target_col]\nX_test_tree = test_lag.drop(columns=[target_col])\ny_test = test_lag[target_col]\n# ---------- 4. Scaling for deep learning ----------\nSEQ_LEN = 24\n# Include cyclical hour/month encoding for sequence models\nfeatures = [\n    \"hour\",\n    \"month\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_train_tree",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_train_tree = train_lag[target_col]\nX_test_tree = test_lag.drop(columns=[target_col])\ny_test = test_lag[target_col]\n# ---------- 4. Scaling for deep learning ----------\nSEQ_LEN = 24\n# Include cyclical hour/month encoding for sequence models\nfeatures = [\n    \"hour\",\n    \"month\",\n    \"hour_sin\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "X_test_tree",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "X_test_tree = test_lag.drop(columns=[target_col])\ny_test = test_lag[target_col]\n# ---------- 4. Scaling for deep learning ----------\nSEQ_LEN = 24\n# Include cyclical hour/month encoding for sequence models\nfeatures = [\n    \"hour\",\n    \"month\",\n    \"hour_sin\",\n    \"hour_cos\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_test",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_test = test_lag[target_col]\n# ---------- 4. Scaling for deep learning ----------\nSEQ_LEN = 24\n# Include cyclical hour/month encoding for sequence models\nfeatures = [\n    \"hour\",\n    \"month\",\n    \"hour_sin\",\n    \"hour_cos\",\n    \"month_sin\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "SEQ_LEN",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "SEQ_LEN = 24\n# Include cyclical hour/month encoding for sequence models\nfeatures = [\n    \"hour\",\n    \"month\",\n    \"hour_sin\",\n    \"hour_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"Temperature\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "features",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "features = [\n    \"hour\",\n    \"month\",\n    \"hour_sin\",\n    \"hour_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"Temperature\",\n    'SolarZenith',\n    \"HumiditySpecific\",",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "df_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "df_seq = df_day[features].dropna()\ntrain_seq_df = df_seq.loc[: train.index.max()]\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\nscaler_X.fit(train_seq_df.drop(columns=[target_col]))\nscaler_y.fit(train_seq_df[[target_col]])\n# Save scalers\njoblib.dump(scaler_X, \"saved_models/scaler_X.pkl\")\njoblib.dump(scaler_y, \"saved_models/scaler_y.pkl\")\ndef create_sequences(df_in, seq_len=24):",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "train_seq_df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "train_seq_df = df_seq.loc[: train.index.max()]\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\nscaler_X.fit(train_seq_df.drop(columns=[target_col]))\nscaler_y.fit(train_seq_df[[target_col]])\n# Save scalers\njoblib.dump(scaler_X, \"saved_models/scaler_X.pkl\")\njoblib.dump(scaler_y, \"saved_models/scaler_y.pkl\")\ndef create_sequences(df_in, seq_len=24):\n    Xs, ys, idxs = [], [], []",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "scaler_X",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "scaler_X = StandardScaler()\nscaler_y = StandardScaler()\nscaler_X.fit(train_seq_df.drop(columns=[target_col]))\nscaler_y.fit(train_seq_df[[target_col]])\n# Save scalers\njoblib.dump(scaler_X, \"saved_models/scaler_X.pkl\")\njoblib.dump(scaler_y, \"saved_models/scaler_y.pkl\")\ndef create_sequences(df_in, seq_len=24):\n    Xs, ys, idxs = [], [], []\n    X_arr = scaler_X.transform(df_in.drop(columns=[target_col]))",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "scaler_y",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "scaler_y = StandardScaler()\nscaler_X.fit(train_seq_df.drop(columns=[target_col]))\nscaler_y.fit(train_seq_df[[target_col]])\n# Save scalers\njoblib.dump(scaler_X, \"saved_models/scaler_X.pkl\")\njoblib.dump(scaler_y, \"saved_models/scaler_y.pkl\")\ndef create_sequences(df_in, seq_len=24):\n    Xs, ys, idxs = [], [], []\n    X_arr = scaler_X.transform(df_in.drop(columns=[target_col]))\n    y_arr = scaler_y.transform(df_in[[target_col]]).flatten()",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "train_idx_set",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "train_idx_set = set(train.index)\ntest_idx_set = set(test.index)\ntrain_mask = np.array([idx in train_idx_set for idx in idxs_all])\ntest_mask = np.array([idx in test_idx_set for idx in idxs_all])\nX_train_seq = X_seq_all[train_mask]\ny_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "test_idx_set",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "test_idx_set = set(test.index)\ntrain_mask = np.array([idx in train_idx_set for idx in idxs_all])\ntest_mask = np.array([idx in test_idx_set for idx in idxs_all])\nX_train_seq = X_seq_all[train_mask]\ny_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "train_mask",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "train_mask = np.array([idx in train_idx_set for idx in idxs_all])\ntest_mask = np.array([idx in test_idx_set for idx in idxs_all])\nX_train_seq = X_seq_all[train_mask]\ny_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "test_mask",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "test_mask = np.array([idx in test_idx_set for idx in idxs_all])\nX_train_seq = X_seq_all[train_mask]\ny_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "X_train_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "X_train_seq = X_seq_all[train_mask]\ny_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_train_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_train_seq = y_seq_all[train_mask]\nX_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost\n# ======================================",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "X_test_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "X_test_seq = X_seq_all[test_mask]\ny_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost\n# ======================================\nprint(\"\\n=== Training XGBoost ===\")",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_test_seq",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_test_seq = y_seq_all[test_mask]\ny_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost\n# ======================================\nprint(\"\\n=== Training XGBoost ===\")\nxgb_model = xgb.XGBRegressor(",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_train_unscaled",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()\ny_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost\n# ======================================\nprint(\"\\n=== Training XGBoost ===\")\nxgb_model = xgb.XGBRegressor(\n    n_estimators=800,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "y_test_unscaled",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "y_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n# ---------- RESULTS ----------\nresults = []\n# ======================================\n# 6. XGBoost\n# ======================================\nprint(\"\\n=== Training XGBoost ===\")\nxgb_model = xgb.XGBRegressor(\n    n_estimators=800,\n    max_depth=5,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "results = []\n# ======================================\n# 6. XGBoost\n# ======================================\nprint(\"\\n=== Training XGBoost ===\")\nxgb_model = xgb.XGBRegressor(\n    n_estimators=800,\n    max_depth=5,\n    learning_rate=0.05,\n    subsample=0.8,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "xgb_model",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "xgb_model = xgb.XGBRegressor(\n    n_estimators=800,\n    max_depth=5,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=RANDOM_STATE,\n    n_jobs=-1,\n)\nxgb_model.fit(X_train_tree, y_train_tree)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "xgb_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "xgb_train_pred = xgb_model.predict(X_train_tree)\nxgb_test_pred = xgb_model.predict(X_test_tree)\nresults.append(\n    [\n        \"XGBoost\",\n        r2_score(y_train_tree, xgb_train_pred),\n        r2_score(y_test, xgb_test_pred),\n        mean_absolute_error(y_test, xgb_test_pred),\n        rmse(y_test, xgb_test_pred),\n    ]",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "xgb_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "xgb_test_pred = xgb_model.predict(X_test_tree)\nresults.append(\n    [\n        \"XGBoost\",\n        r2_score(y_train_tree, xgb_train_pred),\n        r2_score(y_test, xgb_test_pred),\n        mean_absolute_error(y_test, xgb_test_pred),\n        rmse(y_test, xgb_test_pred),\n    ]\n)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "rf",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "rf = RandomForestRegressor(\n    n_estimators=300, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1\n)\nrf.fit(X_train_tree, y_train_tree)\n# Save model\njoblib.dump(rf, \"saved_models/random_forest_model.pkl\")\nrf_train_pred = rf.predict(X_train_tree)\nrf_test_pred = rf.predict(X_test_tree)\nresults.append(\n    [",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "rf_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "rf_train_pred = rf.predict(X_train_tree)\nrf_test_pred = rf.predict(X_test_tree)\nresults.append(\n    [\n        \"Random Forest\",\n        r2_score(y_train_tree, rf_train_pred),\n        r2_score(y_test, rf_test_pred),\n        mean_absolute_error(y_test, rf_test_pred),\n        rmse(y_test, rf_test_pred),\n    ]",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "rf_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "rf_test_pred = rf.predict(X_test_tree)\nresults.append(\n    [\n        \"Random Forest\",\n        r2_score(y_train_tree, rf_train_pred),\n        r2_score(y_test, rf_test_pred),\n        mean_absolute_error(y_test, rf_test_pred),\n        rmse(y_test, rf_test_pred),\n    ]\n)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lstm_model",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lstm_model = models.Sequential(\n    [\n        layers.Input(shape=(SEQ_LEN, X_train_seq.shape[2])),\n        layers.LSTM(64),\n        layers.Dropout(0.2),\n        layers.Dense(32, activation=\"relu\"),\n        layers.Dense(1),\n    ]\n)\nlstm_model.compile(optimizer=\"adam\", loss=\"mse\")",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "es",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "es = callbacks.EarlyStopping(patience=5, restore_best_weights=True)\nlstm_model.fit(\n    X_train_seq,\n    y_train_seq,\n    validation_split=0.1,\n    epochs=50,\n    batch_size=64,\n    callbacks=[es],\n    verbose=0,\n)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lstm_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lstm_train_pred = scaler_y.inverse_transform(lstm_model.predict(X_train_seq))\nlstm_test_pred = scaler_y.inverse_transform(lstm_model.predict(X_test_seq))\nlstm_train_pred = lstm_train_pred.flatten()\nlstm_test_pred = lstm_test_pred.flatten()\nresults.append(\n    [\n        \"LSTM\",\n        r2_score(y_train_unscaled, lstm_train_pred),\n        r2_score(y_test_unscaled, lstm_test_pred),\n        mean_absolute_error(y_test_unscaled, lstm_test_pred),",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lstm_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lstm_test_pred = scaler_y.inverse_transform(lstm_model.predict(X_test_seq))\nlstm_train_pred = lstm_train_pred.flatten()\nlstm_test_pred = lstm_test_pred.flatten()\nresults.append(\n    [\n        \"LSTM\",\n        r2_score(y_train_unscaled, lstm_train_pred),\n        r2_score(y_test_unscaled, lstm_test_pred),\n        mean_absolute_error(y_test_unscaled, lstm_test_pred),\n        rmse(y_test_unscaled, lstm_test_pred),",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lstm_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lstm_train_pred = lstm_train_pred.flatten()\nlstm_test_pred = lstm_test_pred.flatten()\nresults.append(\n    [\n        \"LSTM\",\n        r2_score(y_train_unscaled, lstm_train_pred),\n        r2_score(y_test_unscaled, lstm_test_pred),\n        mean_absolute_error(y_test_unscaled, lstm_test_pred),\n        rmse(y_test_unscaled, lstm_test_pred),\n    ]",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "lstm_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "lstm_test_pred = lstm_test_pred.flatten()\nresults.append(\n    [\n        \"LSTM\",\n        r2_score(y_train_unscaled, lstm_train_pred),\n        r2_score(y_test_unscaled, lstm_test_pred),\n        mean_absolute_error(y_test_unscaled, lstm_test_pred),\n        rmse(y_test_unscaled, lstm_test_pred),\n    ]\n)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "inp",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "inp = layers.Input(shape=(SEQ_LEN, X_train_seq.shape[2]))\nx = layers.Conv1D(32, 3, padding=\"same\", activation=\"relu\")(inp)\nx = layers.MaxPool1D(2)(x)\nx = layers.LSTM(48)(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(24, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\ncnn_lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\ncnn_lstm_model.fit(",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Conv1D(32, 3, padding=\"same\", activation=\"relu\")(inp)\nx = layers.MaxPool1D(2)(x)\nx = layers.LSTM(48)(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(24, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\ncnn_lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\ncnn_lstm_model.fit(\n    X_train_seq,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.MaxPool1D(2)(x)\nx = layers.LSTM(48)(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(24, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\ncnn_lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\ncnn_lstm_model.fit(\n    X_train_seq,\n    y_train_seq,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.LSTM(48)(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(24, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\ncnn_lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\ncnn_lstm_model.fit(\n    X_train_seq,\n    y_train_seq,\n    validation_split=0.1,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Dropout(0.2)(x)\nx = layers.Dense(24, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\ncnn_lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\ncnn_lstm_model.fit(\n    X_train_seq,\n    y_train_seq,\n    validation_split=0.1,\n    epochs=50,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "x = layers.Dense(24, activation=\"relu\")(x)\nout = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\ncnn_lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\ncnn_lstm_model.fit(\n    X_train_seq,\n    y_train_seq,\n    validation_split=0.1,\n    epochs=50,\n    batch_size=64,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "out",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "out = layers.Dense(1)(x)\ncnn_lstm_model = models.Model(inp, out)\ncnn_lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\ncnn_lstm_model.fit(\n    X_train_seq,\n    y_train_seq,\n    validation_split=0.1,\n    epochs=50,\n    batch_size=64,\n    callbacks=[es],",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "cnn_lstm_model",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "cnn_lstm_model = models.Model(inp, out)\ncnn_lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\ncnn_lstm_model.fit(\n    X_train_seq,\n    y_train_seq,\n    validation_split=0.1,\n    epochs=50,\n    batch_size=64,\n    callbacks=[es],\n    verbose=0,",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "cnn_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "cnn_train_pred = scaler_y.inverse_transform(cnn_lstm_model.predict(X_train_seq))\ncnn_test_pred = scaler_y.inverse_transform(cnn_lstm_model.predict(X_test_seq))\ncnn_train_pred = cnn_train_pred.flatten()\ncnn_test_pred = cnn_test_pred.flatten()\nresults.append(\n    [\n        \"CNN-LSTM\",\n        r2_score(y_train_unscaled, cnn_train_pred),\n        r2_score(y_test_unscaled, cnn_test_pred),\n        mean_absolute_error(y_test_unscaled, cnn_test_pred),",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "cnn_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "cnn_test_pred = scaler_y.inverse_transform(cnn_lstm_model.predict(X_test_seq))\ncnn_train_pred = cnn_train_pred.flatten()\ncnn_test_pred = cnn_test_pred.flatten()\nresults.append(\n    [\n        \"CNN-LSTM\",\n        r2_score(y_train_unscaled, cnn_train_pred),\n        r2_score(y_test_unscaled, cnn_test_pred),\n        mean_absolute_error(y_test_unscaled, cnn_test_pred),\n        rmse(y_test_unscaled, cnn_test_pred),",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "cnn_train_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "cnn_train_pred = cnn_train_pred.flatten()\ncnn_test_pred = cnn_test_pred.flatten()\nresults.append(\n    [\n        \"CNN-LSTM\",\n        r2_score(y_train_unscaled, cnn_train_pred),\n        r2_score(y_test_unscaled, cnn_test_pred),\n        mean_absolute_error(y_test_unscaled, cnn_test_pred),\n        rmse(y_test_unscaled, cnn_test_pred),\n    ]",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "cnn_test_pred",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "cnn_test_pred = cnn_test_pred.flatten()\nresults.append(\n    [\n        \"CNN-LSTM\",\n        r2_score(y_train_unscaled, cnn_train_pred),\n        r2_score(y_test_unscaled, cnn_test_pred),\n        mean_absolute_error(y_test_unscaled, cnn_test_pred),\n        rmse(y_test_unscaled, cnn_test_pred),\n    ]\n)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "results_df",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "results_df = pd.DataFrame(\n    results, columns=[\"Model\", \"R2_Train\", \"R2_Test\", \"MAE_Test\", \"RMSE_Test\"]\n)\nprint(results_df)\nresults_df.to_csv(\"saved_models/model_results.csv\", index=False)\nprint(\"\\nðŸŽ‰ All models saved successfully in: saved_models/\")",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom tensorflow.keras.models import load_model\nimport matplotlib.pyplot as plt\n# ---------- 1. Load historical data ----------\ndf = pd.read_csv(\n    \"NASA meteriological and solar radiaton data/lahore_hourly_filled.csv\",\n    parse_dates=[\"datetime\"],",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df = pd.read_csv(\n    \"NASA meteriological and solar radiaton data/lahore_hourly_filled.csv\",\n    parse_dates=[\"datetime\"],\n    dayfirst=True,\n    index_col=\"datetime\",\n)\ndf = df.sort_index().apply(pd.to_numeric, errors=\"coerce\")\ndf.index = pd.to_datetime(df.index, dayfirst=True)\n# Use ALL hours (not just daylight) - model should know night = 0\ndf_day = df.copy()",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df = df.sort_index().apply(pd.to_numeric, errors=\"coerce\")\ndf.index = pd.to_datetime(df.index, dayfirst=True)\n# Use ALL hours (not just daylight) - model should know night = 0\ndf_day = df.copy()\ndf_day.dropna(subset=[\"SolarRadiation\"], inplace=True)\n# Add cyclical encoding for hour and month\ndf_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df.index",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df.index = pd.to_datetime(df.index, dayfirst=True)\n# Use ALL hours (not just daylight) - model should know night = 0\ndf_day = df.copy()\ndf_day.dropna(subset=[\"SolarRadiation\"], inplace=True)\n# Add cyclical encoding for hour and month\ndf_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day = df.copy()\ndf_day.dropna(subset=[\"SolarRadiation\"], inplace=True)\n# Add cyclical encoding for hour and month\ndf_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"hour\"] = df_day.index.hour\ndf_day[\"month\"] = df_day.index.month\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\nxgb_model = joblib.load(\"save_model/xgboost_model.pkl\")\nrf_model = joblib.load(\"save_model/random_forest_model.pkl\")",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"month\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"month\"] = df_day.index.month\ndf_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\nxgb_model = joblib.load(\"save_model/xgboost_model.pkl\")\nrf_model = joblib.load(\"save_model/random_forest_model.pkl\")\nlstm_model = load_model(\"save_model/lstm_model.h5\")",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour_sin\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"hour_sin\"] = np.sin(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\nxgb_model = joblib.load(\"save_model/xgboost_model.pkl\")\nrf_model = joblib.load(\"save_model/random_forest_model.pkl\")\nlstm_model = load_model(\"save_model/lstm_model.h5\")\ncnn_lstm_model = load_model(\"save_model/cnn_lstm_model.h5\")",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"hour_cos\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"hour_cos\"] = np.cos(2 * np.pi * df_day[\"hour\"] / 24)\ndf_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\nxgb_model = joblib.load(\"save_model/xgboost_model.pkl\")\nrf_model = joblib.load(\"save_model/random_forest_model.pkl\")\nlstm_model = load_model(\"save_model/lstm_model.h5\")\ncnn_lstm_model = load_model(\"save_model/cnn_lstm_model.h5\")\nscaler_X = joblib.load(\"save_model/scaler_X.pkl\")",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"month_sin\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"month_sin\"] = np.sin(2 * np.pi * df_day[\"month\"] / 12)\ndf_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\nxgb_model = joblib.load(\"save_model/xgboost_model.pkl\")\nrf_model = joblib.load(\"save_model/random_forest_model.pkl\")\nlstm_model = load_model(\"save_model/lstm_model.h5\")\ncnn_lstm_model = load_model(\"save_model/cnn_lstm_model.h5\")\nscaler_X = joblib.load(\"save_model/scaler_X.pkl\")\nscaler_y = joblib.load(\"save_model/scaler_y.pkl\")",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "df_day[\"month_cos\"]",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "df_day[\"month_cos\"] = np.cos(2 * np.pi * df_day[\"month\"] / 12)\ntarget_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\nxgb_model = joblib.load(\"save_model/xgboost_model.pkl\")\nrf_model = joblib.load(\"save_model/random_forest_model.pkl\")\nlstm_model = load_model(\"save_model/lstm_model.h5\")\ncnn_lstm_model = load_model(\"save_model/cnn_lstm_model.h5\")\nscaler_X = joblib.load(\"save_model/scaler_X.pkl\")\nscaler_y = joblib.load(\"save_model/scaler_y.pkl\")\n# ---------- 3. Forecast settings ----------",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "target_col",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "target_col = \"SolarRadiation\"\n# ---------- 2. Load trained models ----------\nxgb_model = joblib.load(\"save_model/xgboost_model.pkl\")\nrf_model = joblib.load(\"save_model/random_forest_model.pkl\")\nlstm_model = load_model(\"save_model/lstm_model.h5\")\ncnn_lstm_model = load_model(\"save_model/cnn_lstm_model.h5\")\nscaler_X = joblib.load(\"save_model/scaler_X.pkl\")\nscaler_y = joblib.load(\"save_model/scaler_y.pkl\")\n# ---------- 3. Forecast settings ----------\nMAX_LAG = 24",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "xgb_model",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "xgb_model = joblib.load(\"save_model/xgboost_model.pkl\")\nrf_model = joblib.load(\"save_model/random_forest_model.pkl\")\nlstm_model = load_model(\"save_model/lstm_model.h5\")\ncnn_lstm_model = load_model(\"save_model/cnn_lstm_model.h5\")\nscaler_X = joblib.load(\"save_model/scaler_X.pkl\")\nscaler_y = joblib.load(\"save_model/scaler_y.pkl\")\n# ---------- 3. Forecast settings ----------\nMAX_LAG = 24\nSEQ_LEN = 24\n# ---------- 4. User input: day to forecast ----------",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "rf_model",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "rf_model = joblib.load(\"save_model/random_forest_model.pkl\")\nlstm_model = load_model(\"save_model/lstm_model.h5\")\ncnn_lstm_model = load_model(\"save_model/cnn_lstm_model.h5\")\nscaler_X = joblib.load(\"save_model/scaler_X.pkl\")\nscaler_y = joblib.load(\"save_model/scaler_y.pkl\")\n# ---------- 3. Forecast settings ----------\nMAX_LAG = 24\nSEQ_LEN = 24\n# ---------- 4. User input: day to forecast ----------\nyear = 2025",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "lstm_model",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "lstm_model = load_model(\"save_model/lstm_model.h5\")\ncnn_lstm_model = load_model(\"save_model/cnn_lstm_model.h5\")\nscaler_X = joblib.load(\"save_model/scaler_X.pkl\")\nscaler_y = joblib.load(\"save_model/scaler_y.pkl\")\n# ---------- 3. Forecast settings ----------\nMAX_LAG = 24\nSEQ_LEN = 24\n# ---------- 4. User input: day to forecast ----------\nyear = 2025\nmonth = 6  # June",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "cnn_lstm_model",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "cnn_lstm_model = load_model(\"save_model/cnn_lstm_model.h5\")\nscaler_X = joblib.load(\"save_model/scaler_X.pkl\")\nscaler_y = joblib.load(\"save_model/scaler_y.pkl\")\n# ---------- 3. Forecast settings ----------\nMAX_LAG = 24\nSEQ_LEN = 24\n# ---------- 4. User input: day to forecast ----------\nyear = 2025\nmonth = 6  # June\nday = 15  # 15th",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "scaler_X",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "scaler_X = joblib.load(\"save_model/scaler_X.pkl\")\nscaler_y = joblib.load(\"save_model/scaler_y.pkl\")\n# ---------- 3. Forecast settings ----------\nMAX_LAG = 24\nSEQ_LEN = 24\n# ---------- 4. User input: day to forecast ----------\nyear = 2025\nmonth = 6  # June\nday = 15  # 15th\n# Generate hourly datetime index for the day",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "scaler_y",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "scaler_y = joblib.load(\"save_model/scaler_y.pkl\")\n# ---------- 3. Forecast settings ----------\nMAX_LAG = 24\nSEQ_LEN = 24\n# ---------- 4. User input: day to forecast ----------\nyear = 2025\nmonth = 6  # June\nday = 15  # 15th\n# Generate hourly datetime index for the day\ndate_index = pd.date_range(",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "MAX_LAG",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "MAX_LAG = 24\nSEQ_LEN = 24\n# ---------- 4. User input: day to forecast ----------\nyear = 2025\nmonth = 6  # June\nday = 15  # 15th\n# Generate hourly datetime index for the day\ndate_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",\n    end=f\"{year}-{month:02d}-{day:02d} 23:00\",",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "SEQ_LEN",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "SEQ_LEN = 24\n# ---------- 4. User input: day to forecast ----------\nyear = 2025\nmonth = 6  # June\nday = 15  # 15th\n# Generate hourly datetime index for the day\ndate_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",\n    end=f\"{year}-{month:02d}-{day:02d} 23:00\",\n    freq=\"H\",",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "year",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "year = 2025\nmonth = 6  # June\nday = 15  # 15th\n# Generate hourly datetime index for the day\ndate_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",\n    end=f\"{year}-{month:02d}-{day:02d} 23:00\",\n    freq=\"H\",\n)\nhours = date_index.hour",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "month",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "month = 6  # June\nday = 15  # 15th\n# Generate hourly datetime index for the day\ndate_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",\n    end=f\"{year}-{month:02d}-{day:02d} 23:00\",\n    freq=\"H\",\n)\nhours = date_index.hour\n# ---------- 5. Prepare tree-based model inputs ----------",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "day",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "day = 15  # 15th\n# Generate hourly datetime index for the day\ndate_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",\n    end=f\"{year}-{month:02d}-{day:02d} 23:00\",\n    freq=\"H\",\n)\nhours = date_index.hour\n# ---------- 5. Prepare tree-based model inputs ----------\nlast_hist_values = df_day[target_col].values",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "date_index",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "date_index = pd.date_range(\n    start=f\"{year}-{month:02d}-{day:02d} 00:00\",\n    end=f\"{year}-{month:02d}-{day:02d} 23:00\",\n    freq=\"H\",\n)\nhours = date_index.hour\n# ---------- 5. Prepare tree-based model inputs ----------\nlast_hist_values = df_day[target_col].values\ntree_series = last_hist_values[-MAX_LAG:].tolist()  # last 24 hours\n# Use last known weather values",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "hours",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "hours = date_index.hour\n# ---------- 5. Prepare tree-based model inputs ----------\nlast_hist_values = df_day[target_col].values\ntree_series = last_hist_values[-MAX_LAG:].tolist()  # last 24 hours\n# Use last known weather values\nTemperature = df_day[\"Temperature\"].iloc[-1]\nSolarZenith = df_day[\"SolarZenith\"].iloc[-1]\nHumiditySpecific = df_day[\"HumiditySpecific\"].iloc[-1]\nHumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "last_hist_values",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "last_hist_values = df_day[target_col].values\ntree_series = last_hist_values[-MAX_LAG:].tolist()  # last 24 hours\n# Use last known weather values\nTemperature = df_day[\"Temperature\"].iloc[-1]\nSolarZenith = df_day[\"SolarZenith\"].iloc[-1]\nHumiditySpecific = df_day[\"HumiditySpecific\"].iloc[-1]\nHumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "tree_series",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "tree_series = last_hist_values[-MAX_LAG:].tolist()  # last 24 hours\n# Use last known weather values\nTemperature = df_day[\"Temperature\"].iloc[-1]\nSolarZenith = df_day[\"SolarZenith\"].iloc[-1]\nHumiditySpecific = df_day[\"HumiditySpecific\"].iloc[-1]\nHumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\ntree_predictions = []",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "Temperature",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "Temperature = df_day[\"Temperature\"].iloc[-1]\nSolarZenith = df_day[\"SolarZenith\"].iloc[-1]\nHumiditySpecific = df_day[\"HumiditySpecific\"].iloc[-1]\nHumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\ntree_predictions = []\nfor h in range(24):\n    hour = hours[h]",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "SolarZenith",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "SolarZenith = df_day[\"SolarZenith\"].iloc[-1]\nHumiditySpecific = df_day[\"HumiditySpecific\"].iloc[-1]\nHumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\ntree_predictions = []\nfor h in range(24):\n    hour = hours[h]\n    # Compute cyclical features for current hour",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "HumiditySpecific",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "HumiditySpecific = df_day[\"HumiditySpecific\"].iloc[-1]\nHumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\ntree_predictions = []\nfor h in range(24):\n    hour = hours[h]\n    # Compute cyclical features for current hour\n    hour_sin = np.sin(2 * np.pi * hour / 24)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "HumidityRelative",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "HumidityRelative = df_day[\"HumidityRelative\"].iloc[-1]\nPressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\ntree_predictions = []\nfor h in range(24):\n    hour = hours[h]\n    # Compute cyclical features for current hour\n    hour_sin = np.sin(2 * np.pi * hour / 24)\n    hour_cos = np.cos(2 * np.pi * hour / 24)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "Pressure",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "Pressure = df_day[\"Pressure\"].iloc[-1]\nWindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\ntree_predictions = []\nfor h in range(24):\n    hour = hours[h]\n    # Compute cyclical features for current hour\n    hour_sin = np.sin(2 * np.pi * hour / 24)\n    hour_cos = np.cos(2 * np.pi * hour / 24)\n    month_sin = np.sin(2 * np.pi * month / 12)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "WindSpeed",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "WindSpeed = df_day[\"WindSpeed\"].iloc[-1]\nWindDirection = df_day[\"WindDirection\"].iloc[-1]\ntree_predictions = []\nfor h in range(24):\n    hour = hours[h]\n    # Compute cyclical features for current hour\n    hour_sin = np.sin(2 * np.pi * hour / 24)\n    hour_cos = np.cos(2 * np.pi * hour / 24)\n    month_sin = np.sin(2 * np.pi * month / 12)\n    month_cos = np.cos(2 * np.pi * month / 12)",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "WindDirection",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "WindDirection = df_day[\"WindDirection\"].iloc[-1]\ntree_predictions = []\nfor h in range(24):\n    hour = hours[h]\n    # Compute cyclical features for current hour\n    hour_sin = np.sin(2 * np.pi * hour / 24)\n    hour_cos = np.cos(2 * np.pi * hour / 24)\n    month_sin = np.sin(2 * np.pi * month / 12)\n    month_cos = np.cos(2 * np.pi * month / 12)\n    lag_feats = tree_series[-MAX_LAG:]",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "tree_predictions",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "tree_predictions = []\nfor h in range(24):\n    hour = hours[h]\n    # Compute cyclical features for current hour\n    hour_sin = np.sin(2 * np.pi * hour / 24)\n    hour_cos = np.cos(2 * np.pi * hour / 24)\n    month_sin = np.sin(2 * np.pi * month / 12)\n    month_cos = np.cos(2 * np.pi * month / 12)\n    lag_feats = tree_series[-MAX_LAG:]\n    # Features order must match training: hour, month, hour_sin, hour_cos, month_sin, month_cos, weather, lags",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "seq_features",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "seq_features = [\n    \"hour\",\n    \"month\",\n    \"hour_sin\",\n    \"hour_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"Temperature\",\n    \"SolarZenith\",\n    \"HumiditySpecific\",",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "seq_data",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "seq_data = df_day[seq_features].copy()\n# Scale features (excluding target which is last column)\nX_seq_scaled = scaler_X.transform(seq_data.drop(columns=[target_col])).astype(\n    np.float32\n)\n# Start with last SEQ_LEN rows\nseq_array = X_seq_scaled[-SEQ_LEN:].copy()\nseq_predictions = []\n# Get the feature indices for hour-related features (for updating during prediction)\n# Feature order: hour, month, hour_sin, hour_cos, month_sin, month_cos, weather...",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "X_seq_scaled",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "X_seq_scaled = scaler_X.transform(seq_data.drop(columns=[target_col])).astype(\n    np.float32\n)\n# Start with last SEQ_LEN rows\nseq_array = X_seq_scaled[-SEQ_LEN:].copy()\nseq_predictions = []\n# Get the feature indices for hour-related features (for updating during prediction)\n# Feature order: hour, month, hour_sin, hour_cos, month_sin, month_cos, weather...\n# We need to update these for each predicted hour\nfor h in range(24):",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "seq_array",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "seq_array = X_seq_scaled[-SEQ_LEN:].copy()\nseq_predictions = []\n# Get the feature indices for hour-related features (for updating during prediction)\n# Feature order: hour, month, hour_sin, hour_cos, month_sin, month_cos, weather...\n# We need to update these for each predicted hour\nfor h in range(24):\n    X_seq_input = seq_array[-SEQ_LEN:].reshape(1, SEQ_LEN, seq_array.shape[1])\n    lstm_pred_scaled = lstm_model.predict(X_seq_input, verbose=0)\n    cnn_pred_scaled = cnn_lstm_model.predict(X_seq_input, verbose=0)\n    lstm_pred = scaler_y.inverse_transform(lstm_pred_scaled.reshape(-1, 1))[0, 0]",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "seq_predictions",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "seq_predictions = []\n# Get the feature indices for hour-related features (for updating during prediction)\n# Feature order: hour, month, hour_sin, hour_cos, month_sin, month_cos, weather...\n# We need to update these for each predicted hour\nfor h in range(24):\n    X_seq_input = seq_array[-SEQ_LEN:].reshape(1, SEQ_LEN, seq_array.shape[1])\n    lstm_pred_scaled = lstm_model.predict(X_seq_input, verbose=0)\n    cnn_pred_scaled = cnn_lstm_model.predict(X_seq_input, verbose=0)\n    lstm_pred = scaler_y.inverse_transform(lstm_pred_scaled.reshape(-1, 1))[0, 0]\n    cnn_pred = scaler_y.inverse_transform(cnn_pred_scaled.reshape(-1, 1))[0, 0]",
        "detail": "trend",
        "documentation": {}
    },
    {
        "label": "results_day",
        "kind": 5,
        "importPath": "trend",
        "description": "trend",
        "peekOfCode": "results_day = pd.DataFrame(\n    {\n        \"datetime\": date_index,\n        \"XGBoost\": [x for x, _ in tree_predictions],\n        \"RandomForest\": [x for _, x in tree_predictions],\n        \"LSTM\": [x for x, _ in seq_predictions],\n        \"CNN_LSTM\": [x for _, x in seq_predictions],\n    }\n)\n# ---------- 8. Save to CSV ----------",
        "detail": "trend",
        "documentation": {}
    }
]